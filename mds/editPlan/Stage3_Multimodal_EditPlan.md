# Uni-Sign Stage 3 å¤šæ¨¡æ€æ”¹è¿›æ–¹æ¡ˆ - åˆ†æ­¥ä¿®æ”¹é€»è¾‘

**æ–¹æ¡ˆåˆ›å»ºæ—¥æœŸ**: 2026-02-16  
**æ–¹æ¡ˆç‰ˆæœ¬**: Stage 3 - Phase 1 (Gating Fusion)  
**æ–‡æ¡£ç›®æ ‡**: æä¾›å®Œæ•´çš„åˆ†æ­¥ä»£ç ä¿®æ”¹é€»è¾‘ï¼Œä¸ç ´åç°æœ‰åŠŸèƒ½ï¼Œé€æ­¥é›†æˆåŠ¨ä½œæè¿°æ–‡æœ¬

---

## ğŸ“‹ æ ¸å¿ƒæ–¹æ¡ˆè¦æ±‚

### æŠ€æœ¯è§„æ ¼
| æ¨¡å— | æ–¹æ¡ˆ | è¯´æ˜ |
|------|------|------|
| **æ–‡æœ¬ç¼–ç ** | mT5-base | ç¼–ç  description/ æ–‡ä»¶å¤¹ä¸­çš„åŠ¨ä½œæè¿°æ–‡æœ¬ |
| **æ—¶é—´å¯¹é½** | æ™ºèƒ½æ’å€¼ | ç¼ºå¤±å¸§ç”¨æœ€è¿‘é‚»ï¼Œä¸¤è¾¹éƒ½æœ‰åˆ™çº¿æ€§æ’å€¼ |
| **èåˆæœºåˆ¶** | Gating | åŠ¨æ€æƒé‡èåˆï¼Œè½»é‡çº§ï¼Œå‚æ•°å°‘ |
| **ç¼ºå¤±å¤„ç†** | å¯å­¦ä¹ æ©ç +Dropout | å ä½ç¬¦ã€Text Dropoutã€ç¼ºå¤±æŒ‡ç¤ºç¬¦ |

---

## ğŸ—ï¸ æ€»ä½“æ¶æ„è®¾è®¡

### æ•°æ®æµå›¾
```
è¾“å…¥æ•°æ® 
  â”œâ”€ åŸæœ‰ï¼šè§†é¢‘/å§¿æ€ç‰¹å¾ (B, 4, T, 150)
  â”œâ”€ æ–°å¢ï¼šæè¿°æ–‡æœ¬ (list of str or None)
  â””â”€ æ–°å¢ï¼šå¸§ç´¢å¼•æ˜ å°„ (__frame_indices__)
     â†“
æ•°æ®åŠ è½½å±‚ (datasets.pyæ”¹åŠ¨)
  â”œâ”€ åŠ è½½CSL_Dailyä¸­çš„æè¿°æ–‡æœ¬ (description/CSL_Daily/)
  â”œâ”€ è§£æå¸§ç´¢å¼•å…³ç³» (åŸå§‹å¸§â†’é‡‡æ ·å¸§)
  â”œâ”€ ç”Ÿæˆç¼ºå¤±æŒ‡ç¤ºç¬¦ (has_description)
  â””â”€ æ”¯æŒéƒ¨åˆ†ç¼ºå¤±çš„ä¼˜é›…å¤„ç†
     â†“
æ¨¡å‹å‰å‘ä¼ æ’­
  â”œâ”€ pose features: (B, T, 768) â† åŸæœ‰ç‰¹å¾
  â”œâ”€ text features: (B, T, 768) â† mT5ç¼–ç 
  â”œâ”€ mask_embedding: (1, 768) â† å¯å­¦ä¹ å ä½ç¬¦
  â””â”€ has_description: (B, T, 1) â† ç¼ºå¤±æŒ‡ç¤ºç¬¦
     â†“
èåˆå±‚ (models.pyæ–°å¢)
  â”œâ”€ text encoder: mT5-baseå†»ç»“
  â”œâ”€ gating fusion: å­¦ä¹ æƒé‡èåˆ
  â””â”€ text dropout: è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒ
     â†“
è¾“å‡ºï¼šèåˆç‰¹å¾ (B, T, 768) â† ç”¨äºä¸‹æ¸¸ä»»åŠ¡
```

### æ–°å¢/ä¿®æ”¹æ–‡ä»¶æ¸…å•
```
â”œâ”€â”€ datasets.py              [ä¿®æ”¹] æ•°æ®åŠ è½½ã€æè¿°è§£æã€å¸§ç´¢å¼•å¤„ç†
â”œâ”€â”€ models.py                [ä¿®æ”¹] æ–‡æœ¬ç¼–ç å™¨ã€èåˆæ¨¡å—é›†æˆ
â”œâ”€â”€ fine_tuning.py           [ä¿®æ”¹] è®­ç»ƒå¾ªç¯æ”¯æŒæ–°è¾“å…¥
â”œâ”€â”€ utils.py                 [ä¿®æ”¹] CLIå‚æ•°æ”¯æŒ
â”œâ”€â”€ temporal_alignment.py     [æ–°å»º] æ—¶é—´å¯¹é½ã€æè¿°åŠ è½½å™¨
â”œâ”€â”€ text_fusion_modules.py    [æ–°å»º] æ–‡æœ¬ç¼–ç å™¨ã€Gatingèåˆæ¨¡å—
â”œâ”€â”€ test_multimodal.py        [æ–°å»º] å•å…ƒæµ‹è¯• (å¯é€‰)
â””â”€â”€ config.py                [ä¿®æ”¹] æ–°å¢é…ç½®é¡¹
```

---

## ğŸ“ åˆ†æ­¥ä¿®æ”¹æ–¹æ¡ˆ

### Step 1: æ–°å»º temporal_alignment.py

**ç›®æ ‡**: æä¾›æè¿°åŠ è½½å’Œæ—¶é—´å¯¹é½çš„æ ¸å¿ƒå·¥å…·

**åŠŸèƒ½æ¨¡å—**:
1. `DescriptionLoader`: ä»descriptionæ–‡ä»¶å¤¹åŠ è½½æè¿°æ–‡æœ¬JSON
2. `TemporalAligner`: å¤„ç†å¸§ç´¢å¼•æ˜ å°„å’Œæ™ºèƒ½æ’å€¼
3. è¾…åŠ©å‡½æ•°: å¤„ç†ç¼ºå¤±ã€æ’å€¼ç­‰

**å…³é”®æ¥å£**:
```python
class DescriptionLoader:
    def __init__(self, description_dir):
        """
        åˆå§‹åŒ–æè¿°åŠ è½½å™¨
        Args:
            description_dir: description/CSL_Daily ç›®å½•è·¯å¾„
        """
    
    def load(self, sample_id):
        """
        åŠ è½½å•ä¸ªæ ·æœ¬çš„æè¿°æ–‡æœ¬
        Returns:
            descriptions: dict {frame_id: str} or {}
            metadata: å…ƒæ•°æ®
        """

class TemporalAligner:
    def __init__(self, descriptions, frame_indices):
        """
        æ—¶é—´å¯¹é½å™¨
        Args:
            descriptions: åŸå§‹å¸§çš„æè¿°å­—å…¸
            frame_indices: é‡‡æ ·åçš„å¸§ç´¢å¼•åˆ—è¡¨ [f1, f2, ...]
        """
    
    def align(self):
        """
        è¿›è¡Œæ™ºèƒ½æ’å€¼å¯¹é½
        ç­–ç•¥:
        1. é‡‡æ ·å¸§æœ‰æè¿° â†’ ç›´æ¥ä½¿ç”¨
        2. é‚»è¿‘å¸§æœ‰æè¿° â†’ ä½¿ç”¨æœ€è¿‘å¸§
        3. ä¸¤è¾¹éƒ½æœ‰ â†’ çº¿æ€§æ’å€¼åˆå¹¶
        Returns:
            aligned_descriptions: list (é•¿åº¦=é‡‡æ ·å¸§æ•°)
            has_description: list (ç¼ºå¤±æŒ‡ç¤ºç¬¦)
        """
```

**è¾“å…¥/è¾“å‡ºç¤ºä¾‹**:
```json
// è¾“å…¥: åŸå§‹å¸§æè¿°
{
    "0": "person moves hand to left",
    "2": "hand touches chin",
    "5": "both hands move down"
}

// è¾“å…¥: å¸§é‡‡æ ·ç´¢å¼•
[0, 1, 2, 4, 5]

// è¾“å‡º: å¯¹é½åçš„æè¿° (æ™ºèƒ½æ’å€¼)
[
    "person moves hand to left",      // frame 0: ç›´æ¥
    "person moves hand to left",      // frame 1: æœ€è¿‘é‚»
    "hand touches chin",              // frame 2: ç›´æ¥
    "interpolate(æ‰‹è§¦ä¸‹å·´, åŒæ‰‹å‘ä¸‹)", // frame 4: æ’å€¼
    "both hands move down"            // frame 5: ç›´æ¥
]

// è¾“å‡º: ç¼ºå¤±æŒ‡ç¤ºç¬¦
[1, 1, 1, 0, 1]  // 1=æœ‰æè¿°, 0=æ’å€¼/ç¼ºå¤±
```

**å®ç°æ³¨æ„**:
- å¤„ç†JSONæ ¼å¼çš„æè¿°æ–‡æœ¬ (æ¥è‡ªdescription/ä¸‹çš„JSONæ–‡ä»¶)
- æ”¯æŒå®Œå…¨ç¼ºå¤±çš„æ ·æœ¬ (è¿”å›å…¨None)
- ç¼ºå¤±æŒ‡ç¤ºç¬¦ç”¨äºåç»­æ¨ç†å’ŒGatingåŠ æƒ

---

### Step 2: æ–°å»º text_fusion_modules.py

**ç›®æ ‡**: å®ç°æ–‡æœ¬ç¼–ç å’Œèåˆçš„æ ¸å¿ƒç»„ä»¶

**åŠŸèƒ½æ¨¡å—**:
1. `TextEncoder`: å°è£…mT5-baseçš„æ¨ç†
2. `GatingFusion`: å®ç°Gatingèåˆæœºåˆ¶
3. è¾…åŠ©å‡½æ•°: æ©ç å¤„ç†ç­‰

**å…³é”®ç±»**:

```python
class TextEncoder(nn.Module):
    """mT5-base æ–‡æœ¬ç¼–ç å™¨ (å†»ç»“)"""
    
    def __init__(self, model_name='mt5-base', hidden_dim=768, device='cuda'):
        """
        Args:
            model_name: æ¨¡å‹åç§° (default: mt5-base)
            hidden_dim: è¾“å‡ºç»´åº¦ (ä¸è§†è§‰ç‰¹å¾ä¸€è‡´)
            device: è®¾å¤‡
        """
    
    def forward(self, descriptions, max_length=256):
        """
        å¯¹æè¿°æ–‡æœ¬è¿›è¡Œç¼–ç 
        Args:
            descriptions: list of str (or None elements)
            max_length: æœ€å¤§é•¿åº¦
        
        Returns:
            text_features: (B, hidden_dim) æˆ– (B, T, hidden_dim)
            æœ‰Noneåˆ™è¿”å›å¯¹åº”ä½ç½®çš„é›¶å‘é‡
        """

class GatingFusion(nn.Module):
    """Gating èåˆæœºåˆ¶"""
    
    def __init__(self, feature_dim=768):
        """
        Args:
            feature_dim: ç‰¹å¾ç»´åº¦ (768)
        """
    
    def forward(self, pose_feat, text_feat, has_description, text_dropout_p=0.):
        """
        èåˆè§†é¢‘å§¿æ€å’Œæ–‡æœ¬ç‰¹å¾
        Args:
            pose_feat: (B, T, 768) æˆ– (B, T, C)
            text_feat: (B, T, 768)
            has_description: (B, T, 1) ç¼ºå¤±æŒ‡ç¤ºç¬¦
            text_dropout_p: dropoutæ¦‚ç‡ (è®­ç»ƒæ—¶ä½¿ç”¨)
        
        Returns:
            fused_feat: (B, T, 768)
            gate_weights: (B, T, 1) å¯è§†åŒ–ç”¨
        
        èåˆå…¬å¼:
        gate = Sigmoid(MLP([pose, text, has_description]))
        fused = pose + gate * text
        """

class LearnableMaskEmbedding(nn.Module):
    """å¯å­¦ä¹ æ©ç åµŒå…¥"""
    
    def __init__(self, hidden_dim=768):
        self.mask = nn.Parameter(torch.randn(1, hidden_dim) * 0.01)
    
    def forward(self):
        return self.mask
```

**è®­ç»ƒç­–ç•¥ (Text Dropout)**:
- åœ¨è®­ç»ƒæ—¶ï¼Œä»¥æ¦‚ç‡ `text_dropout_p` (å¦‚0.2) éšæœºæ›¿æ¢æ–‡æœ¬ç‰¹å¾
- æ›¿æ¢ä¸ºæ©ç åµŒå…¥æˆ–é›¶å‘é‡
- æé«˜æ¨¡å‹å¯¹ç¼ºå¤±æ¨¡æ€çš„é²æ£’æ€§

**æ¨ç†ç­–ç•¥ (ç¼ºå¤±æŒ‡ç¤ºç¬¦)**:
- ä½¿ç”¨ `has_description` æ˜¾å¼æŒ‡ç¤ºå“ªäº›å¸§æœ‰çœŸå®æè¿°
- Gatingä¼šè‡ªåŠ¨ä¸ºç¼ºå¤±å¸§åˆ†é…ä½æƒé‡
- å¯é€‰: å®Œå…¨ç¦ç”¨ç¼ºå¤±å¸§çš„æ–‡æœ¬ç‰¹å¾ (ç½®é›¶)

---

### Step 3: ä¿®æ”¹ datasets.py

**ç›®æ ‡**: é›†æˆæè¿°æ–‡æœ¬åŠ è½½å’Œå¸§ç´¢å¼•å¤„ç†

**ä¿®æ”¹ä½ç½®**:
1. **å¯¼å…¥éƒ¨åˆ†** (é¡¶éƒ¨)
   ```python
   from temporal_alignment import DescriptionLoader, TemporalAligner
   from text_fusion_modules import TextEncoder, GatingFusion
   ```

2. **S2T_Dataset ç±»æ”¹åŠ¨**
   - `__init__`: åˆå§‹åŒ– DescriptionLoader
   - `__getitem__`: åŠ è½½æè¿°æ–‡æœ¬ã€è¿›è¡Œæ—¶é—´å¯¹é½
   - `collate_fn`: å¤„ç†æ‰¹é‡æ‰“åŒ…

**å…³é”®ä¿®æ”¹**:

```python
class S2T_Dataset(Dataset):
    def __init__(self, path, args, phase='train'):
        # åŸæœ‰åˆå§‹åŒ–...
        
        # [æ–°å¢] æè¿°åŠ è½½å™¨
        self.use_descriptions = getattr(args, 'use_descriptions', False)
        if self.use_descriptions:
            desc_dir = Path(rgb_dirs[args.dataset]).parent / 'description' / args.dataset
            self.desc_loader = DescriptionLoader(str(desc_dir))
        else:
            self.desc_loader = None
    
    def __getitem__(self, idx):
        # åŸæœ‰é€»è¾‘è¿”å›: src_input, tgt_input, sign_id, sample_path
        src_input, tgt_input, sign_id, sample_path = self._original_getitem(idx)
        
        # [æ–°å¢] åŠ è½½æè¿°æ–‡æœ¬
        if self.use_descriptions and self.desc_loader:
            sample_id = Path(sample_path).stem  # e.g., 'S000196_P0000_T00'
            descriptions, frame_indices = self._load_and_align_descriptions(
                sample_id, src_input
            )
            has_description = torch.tensor(
                [1 if d is not None else 0 
                 for d in descriptions],
                dtype=torch.float32
            )
        else:
            descriptions = None
            frame_indices = None
            has_description = None
        
        return {
            'src_input': src_input,
            'tgt_input': tgt_input,
            'descriptions': descriptions,
            'frame_indices': frame_indices,
            'has_description': has_description,
            'sign_id': sign_id,
            'sample_path': sample_path
        }
    
    def _load_and_align_descriptions(self, sample_id, src_input):
        """
        åŠ è½½æè¿°å¹¶è¿›è¡Œæ—¶é—´å¯¹é½
        """
        try:
            original_descriptions = self.desc_loader.load(sample_id)
            if not original_descriptions:
                return None, None
        except:
            return None, None
        
        # è·å–å¸§ç´¢å¼• (ä»src_inputæ¨æ–­æˆ–å…ƒæ•°æ®)
        # å‡è®¾ src_input çš„æ—¶é—´ç»´åº¦å·²æ˜¯é‡‡æ ·åçš„
        T_sampled = src_input.shape[1] if src_input.dim() >= 2 else 1
        
        aligner = TemporalAligner(original_descriptions, frame_indices=list(range(T_sampled)))
        aligned_descriptions, has_desc = aligner.align()
        
        return aligned_descriptions, list(range(T_sampled))
    
    def collate_fn(self, batch):
        # åŸæœ‰æ‰“åŒ…é€»è¾‘...
        
        # [æ–°å¢] å¤„ç†æè¿°æ–‡æœ¬
        if batch[0].get('descriptions') is not None:
            # æ‰“åŒ…æè¿°å’ŒæŒ‡ç¤ºç¬¦
            descriptions_batch = pad_descriptions([item['descriptions'] for item in batch])
            has_desc_batch = torch.stack([item['has_description'] for item in batch])
        else:
            descriptions_batch = None
            has_desc_batch = None
        
        return {
            'src_input': src_input_packed,
            'tgt_input': tgt_input_packed,
            'descriptions': descriptions_batch,
            'has_description': has_desc_batch,
            # ... å…¶ä»–å­—æ®µ
        }

def pad_descriptions(batch_descriptions):
    """
    å°†ä¸ç­‰é•¿çš„æè¿°åˆ—è¡¨æ‰“åŒ…æˆæ‰¹é‡
    """
    max_len = max(len(desc_list) for desc_list in batch_descriptions)
    padded = []
    for desc_list in batch_descriptions:
        padded_item = desc_list + [None] * (max_len - len(desc_list))
        padded.append(padded_item)
    return padded  # List[List[str or None]]
```

**è¿”å›ç»“æ„å˜åŒ–**:
```python
# åŸæœ‰: tuple (src_input, tgt_input, ...)
# æ–°å¢: dict åŒ…å«å¤šä¸ªå­—æ®µ
{
    'src_input': tensor (B, 4, T, 150),
    'tgt_input': tensor (B, tgt_len),
    'descriptions': List[List[str or None]],  # (B, T)
    'has_description': tensor (B, T, 1),
    'frame_indices': List[List[int]],  # (B, T)
    'sign_id': List[str],
    'sample_path': List[str]
}
```

---

### Step 4: ä¿®æ”¹ models.py

**ç›®æ ‡**: é›†æˆæ–‡æœ¬ç¼–ç å’Œèåˆåˆ°Uni_Signæ¨¡å‹

**ä¿®æ”¹ä½ç½®**:
1. **å¯¼å…¥** (é¡¶éƒ¨)
   ```python
   from text_fusion_modules import TextEncoder, GatingFusion, LearnableMaskEmbedding
   ```

2. **Uni_Sign.__init__** æ–°å¢æ–‡æœ¬å¤„ç†æ¨¡å—
   ```python
   def __init__(self, args):
       super().__init__()
       # åŸæœ‰åˆå§‹åŒ–...
       
       # [æ–°å¢] å¤šæ¨¡æ€èåˆé…ç½®
       self.use_descriptions = getattr(args, 'use_descriptions', False)
       self.text_fusion_type = getattr(args, 'text_fusion_type', 'gating')  # 'gating' or 'cross_attn'
       
       if self.use_descriptions:
           self.text_encoder = TextEncoder(
               model_name='mt5-base',
               hidden_dim=768,
               device=args.device if hasattr(args, 'device') else 'cuda'
           )
           
           self.gating_fusion = GatingFusion(feature_dim=768)
           
           # å¯å­¦ä¹ æ©ç  (ç”¨äºç¼ºå¤±æè¿°)
           self.mask_embedding = LearnableMaskEmbedding(hidden_dim=768)
           
           # Text Dropout æ¦‚ç‡ (è®­ç»ƒæ—¶ä½¿ç”¨)
           self.text_dropout_p = getattr(args, 'text_dropout_p', 0.1)
   ```

3. **Uni_Sign.forward** é›†æˆæ–‡æœ¬èåˆ
   ```python
   def forward(self, src_input, tgt_input, 
               descriptions=None, has_description=None):
       """
       Args:
           src_input: (B, 4, T, 150)
           tgt_input: (B, tgt_len)
           descriptions: List[List[str or None]] (B, T)
           has_description: (B, T, 1)
       
       Returns:
           logits: (B, tgt_len, vocab_size)
       """
       
       # åŸæœ‰è§†è§‰ç¼–ç 
       pose_features = self._encode_pose(src_input)  # (B, T, 768)
       
       # [æ–°å¢] æ–‡æœ¬ç¼–ç å’Œèåˆ
       if self.use_descriptions and descriptions is not None:
           # ç¼–ç æè¿°æ–‡æœ¬
           text_features = self._encode_descriptions(descriptions)  # (B, T, 768)
           
           # åº”ç”¨ Text Dropout (è®­ç»ƒæ—¶)
           if self.training:
               text_features = self._apply_text_dropout(
                   text_features, 
                   has_description,
                   dropout_p=self.text_dropout_p
               )
           
           # Gating èåˆ
           fused_features = self.gating_fusion(
               pose_features,
               text_features,
               has_description
           )
           
           encoder_input = fused_features
       else:
           encoder_input = pose_features
       
       # åç»­å¤„ç† (åŸæœ‰é€»è¾‘)
       encoder_out = self.transformer_encoder(encoder_input)
       decoder_out = self.transformer_decoder(tgt_input, encoder_out)
       logits = self.output_projection(decoder_out)
       
       return logits
   
   def _encode_descriptions(self, descriptions):
       """
       å¯¹æè¿°æ–‡æœ¬è¿›è¡Œç¼–ç 
       """
       B, T = len(descriptions), len(descriptions[0])
       text_features = torch.zeros(B, T, 768, device=self.device)
       mask_emb = self.mask_embedding()  # (1, 768)
       
       for b in range(B):
           for t in range(T):
               if descriptions[b][t] is not None:
                   # ç¼–ç æ–‡æœ¬
                   feat = self.text_encoder([descriptions[b][t]])  # (1, 768)
                   text_features[b, t] = feat[0]
               else:
                   # ä½¿ç”¨æ©ç åµŒå…¥
                   text_features[b, t] = mask_emb.squeeze(0)
       
       return text_features
   
   def _apply_text_dropout(self, text_features, has_description, dropout_p):
       """
       è®­ç»ƒæ—¶åº”ç”¨ Text Dropout
       éšæœºæ›¿æ¢æ–‡æœ¬ç‰¹å¾ä¸ºæ©ç æˆ–é›¶å‘é‡
       """
       if dropout_p <= 0:
           return text_features
       
       # åˆ›å»ºéšæœºæ©ç 
       B, T, D = text_features.shape
       dropout_mask = torch.bernoulli(
           torch.full((B, T, 1), dropout_p, device=text_features.device)
       ).expand(B, T, D)
       
       text_features = text_features * (1 - dropout_mask)
       return text_features
   ```

**å…³é”®è®¾è®¡**:
- mT5 ç¼–ç å™¨å†»ç»“ï¼Œä»…åšç‰¹å¾æå–
- å¯å­¦ä¹ æ©ç ç”¨äºè·å–ç¼ºå¤±å¸§çš„åˆå§‹è¡¨ç¤º
- Gating å­¦ä¹ æœ€ä¼˜æƒé‡èåˆ
- Text Dropout åœ¨è®­ç»ƒæ—¶æé«˜é²æ£’æ€§
- ç¼ºå¤±æŒ‡ç¤ºç¬¦æŒ‡å¯¼ Gating çš„å­¦ä¹ 

---

### Step 5: ä¿®æ”¹ fine_tuning.py

**ç›®æ ‡**: æ”¯æŒæ–°çš„å¤šæ¨¡æ€è¾“å…¥

**ä¿®æ”¹ä½ç½®**:
1. **æ•°æ®åŠ è½½** (main å‡½æ•°ä¸­)
   - ç¡®ä¿ DataLoader è¿”å›æ–°çš„å­—å…¸æ ¼å¼
   - éªŒè¯ collate_fn æ­£å¸¸å·¥ä½œ

2. **è®­ç»ƒå¾ªç¯** (train_epoch å‡½æ•°)
   ```python
   def train_epoch(model, train_dataloader, optimizer, args):
       for batch in train_dataloader:
           src_input = batch['src_input'].to(device)
           tgt_input = batch['tgt_input'].to(device)
           
           # [æ–°å¢] å¤„ç†æè¿°æ–‡æœ¬
           descriptions = batch.get('descriptions', None)
           has_description = batch.get('has_description', None)
           if has_description is not None:
               has_description = has_description.to(device)
           
           # å‰å‘ä¼ æ’­
           if args.use_descriptions:
               logits = model(src_input, tgt_input, 
                            descriptions=descriptions,
                            has_description=has_description)
           else:
               logits = model(src_input, tgt_input)
           
           # è®¡ç®—loss (åŸæœ‰é€»è¾‘)
           loss = criterion(logits, tgt_input)
           
           # åå‘ä¼ æ’­ (åŸæœ‰é€»è¾‘)
           optimizer.zero_grad()
           loss.backward()
           optimizer.step()
   ```

3. **éªŒè¯å¾ªç¯** (eval_step å‡½æ•°)
   - ç±»ä¼¼ä¿®æ”¹ï¼Œç¡®ä¿æ¨ç†æ—¶æ­£ç¡®å¤„ç†å¤šæ¨¡æ€è¾“å…¥
   - ä½¿ç”¨ `model.eval()` ç¦ç”¨ Text Dropout

---

### Step 6: ä¿®æ”¹ utils.py & config.py

**ç›®æ ‡**: æ·»åŠ å‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®

**utils.py æ–°å¢å‚æ•°**:
```python
parser.add_argument('--use_descriptions', 
                    action='store_true', 
                    default=False,
                    help='Enable multimodal fusion with action descriptions')

parser.add_argument('--text_fusion_type', 
                    choices=['gating', 'cross_attn'], 
                    default='gating',
                    help='Fusion mechanism type')

parser.add_argument('--text_dropout_p', 
                    type=float, 
                    default=0.1,
                    help='Text dropout probability during training')

parser.add_argument('--desc_dir', 
                    type=str, 
                    default='description',
                    help='Path to description files')

parser.add_argument('--mt5_model', 
                    type=str, 
                    default='google/mt5-base',
                    help='mT5 model name from HuggingFace')
```

**config.py æ–°å¢é…ç½®**:
```python
# æè¿°æ–‡æœ¬ç›®å½•è·¯å¾„
DESCRIPTION_DIRS = {
    'CSL_Daily': 'description/CSL_Daily',
    'CSL_News': 'description/CSL_News',
    'How2Sign': 'description/How2Sign',
}

# mT5 æ¨¡å‹è·¯å¾„æˆ–åç§°
MT5_MODEL_NAME = 'google/mt5-base'
MT5_HIDDEN_DIM = 768

# èåˆå‚æ•°
TEXT_FUSION_CONFIG = {
    'type': 'gating',                    # 'gating' æˆ– 'cross_attn'
    'text_dropout_p': 0.1,               # Text Dropout æ¦‚ç‡
    'mask_embedding_init_std': 0.01,     # æ©ç åˆå§‹åŒ–
    'gating_hidden_dim': 512,            # Gating MLP éšå±‚ç»´åº¦
}

# æ—¶é—´å¯¹é½å‚æ•°
TEMPORAL_ALIGNMENT_CONFIG = {
    'strategy': 'smart_interpolation',   # æ™ºèƒ½æ’å€¼
    'use_nearest_neighbor': True,        # é‚»è¿‘å¸§ä»£æ›¿
    'use_linear_interpolation': True,    # çº¿æ€§æ’å€¼
}
```

---

## ğŸ” å…³é”®æŠ€æœ¯ç»†èŠ‚

### 1. æ—¶é—´å¯¹é½ (æ™ºèƒ½æ’å€¼)

**é—®é¢˜**: è§†é¢‘è¢«é‡‡æ · (e.g., 25fpsâ†’10fps)ï¼Œæè¿°å¯¹åº”åŸå§‹å¸§å·ï¼Œéœ€è¦æ˜ å°„åˆ°é‡‡æ ·åçš„å¸§å·ã€‚

**ç­–ç•¥**:
```
å¯¹äºé‡‡æ ·åçš„å¸§ i:
  if å¸§içš„åŸå§‹å¸§å· åœ¨ descriptions ä¸­:
      ä½¿ç”¨æè¿°
  elif æœ€è¿‘çš„å¸§æœ‰æè¿°:
      ä½¿ç”¨æœ€è¿‘é‚»æè¿°
  else:
      åœ¨ä¸¤ä¾§æœ‰æè¿°çš„å¸§ä¹‹é—´è¿›è¡Œçº¿æ€§æ’å€¼
      (èåˆä¸¤ä¸ªæè¿°çš„ç‰¹å¾è¡¨ç¤º)
```

**å®ç°**:
```python
def align(self, descriptions_dict, sampled_frame_indices):
    aligned = []
    has_desc = []
    
    for sampled_idx, original_idx in enumerate(sampled_frame_indices):
        if original_idx in descriptions_dict:
            # ç›´æ¥ä½¿ç”¨
            aligned.append(descriptions_dict[original_idx])
            has_desc.append(1)
        else:
            # æŸ¥æ‰¾æœ€è¿‘é‚»
            nearest = find_nearest(original_idx, descriptions_dict.keys())
            if nearest is not None:
                aligned.append(descriptions_dict[nearest])
                has_desc.append(0)  # æ ‡è®°ä¸ºæ’å€¼/é‚»è¿‘
            else:
                aligned.append(None)
                has_desc.append(0)
    
    return aligned, has_desc
```

### 2. å¯å­¦ä¹ æ©ç  (ç¼ºå¤±å ä½ç¬¦)

**è®¾è®¡**:
- åˆå§‹åŒ–: `mask = nn.Parameter(torch.randn(1, 768) * 0.01)`
- ç”¨é€”: ä¸ºç¼ºå¤±çš„æè¿°å¸§æä¾›åˆå§‹è¡¨ç¤º
- è®­ç»ƒ: ä¸æ¨¡å‹å‚æ•°ä¸€èµ·æ›´æ–°

**åº”ç”¨**:
```python
if descriptions[b][t] is None:
    text_features[b][t] = mask_embedding()
```

### 3. Text Dropout (è®­ç»ƒç­–ç•¥)

**ç›®æ ‡**: æé«˜æ¨¡å‹å¯¹ç¼ºå¤±æ¨¡æ€çš„é²æ£’æ€§

**å®ç°**:
```python
if training:
    dropout_mask = torch.bernoulli(torch.full((B,T,1), dropout_p))
    text_features = text_features * (1 - dropout_mask)
```

**æ•ˆæœ**: å¼ºåˆ¶æ¨¡å‹å­¦ä¹ åˆ°å³ä½¿æ–‡æœ¬ç‰¹å¾è¢«éƒ¨åˆ†é®æŒ¡ï¼Œä¹Ÿèƒ½è¿›è¡Œæœ‰æ•ˆèåˆ

### 4. ç¼ºå¤±æŒ‡ç¤ºç¬¦ (æ¨ç†ç­–ç•¥)

**è®¾è®¡**:
- `has_description`: (B, T, 1) tensorï¼Œ1è¡¨ç¤ºæœ‰çœŸå®æè¿°ï¼Œ0è¡¨ç¤ºæ’å€¼/ç¼ºå¤±
- ç”¨äº Gating å­¦ä¹ å“ªäº›å¸§æ›´åº”è¯¥ä¾èµ–æ–‡æœ¬

**åœ¨Gatingä¸­ä½¿ç”¨**:
```python
gate_input = torch.cat([pose_feat, text_feat, has_description], dim=-1)
gate = Sigmoid(MLP(gate_input))
fused = pose_feat + gate * text_feat
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

| æŒ‡æ ‡ | Phase 1 (Gating) | Phase 2 (Cross-Attn) |
|------|------------------|----------------------|
| **BLEUæå‡** | +3-5% | +5-8% |
| **æ¨¡å‹å‚æ•°** | +10M | +50M |
| **æ¨ç†é€Ÿåº¦** | -3% | -8~15% |
| **æ˜¾å­˜å ç”¨** | +2GB | +4GB |
| **è®­ç»ƒæ—¶é—´** | +20% | +40% |

---

## âœ… ä¿®æ”¹éªŒè¯æ¸…å•

### Phase 1: æ•°æ®åŠ è½½éªŒè¯
- [ ] temporal_alignment.py èƒ½æ­£ç¡®åŠ è½½æè¿°æ–‡æœ¬
- [ ] å¸§ç´¢å¼•æ˜ å°„æ­£ç¡® (no off-by-one)
- [ ] ç¼ºå¤±æè¿°çš„æ ·æœ¬ä¸æŠ¥é”™
- [ ] collate_fn æ­£ç¡®æ‰“åŒ…æ‰¹é‡æ•°æ®

### Phase 2: æ¨¡å‹é›†æˆéªŒè¯
- [ ] text_fusion_modules.py ä¸­ mT5 ç¼–ç å™¨èƒ½æ¨ç†
- [ ] æ©ç åµŒå…¥ç”Ÿæˆå½¢çŠ¶æ­£ç¡®
- [ ] Gating Fusion forward pass æ— å½¢çŠ¶é”™è¯¯
- [ ] æ¢¯åº¦èƒ½æ­£ç¡®åå‘ä¼ æ’­

### Phase 3: è®­ç»ƒå¾ªç¯éªŒè¯
- [ ] å•batchè®­ç»ƒæ— OOM
- [ ] Loss æ­£å¸¸ä¸‹é™
- [ ] è¯„ä¼°æŒ‡æ ‡æ­£å¸¸è®¡ç®—
- [ ] å‘åå…¼å®¹: `--use_descriptions=False` æ—¶åŠŸèƒ½ä¸å—å½±å“

### Phase 4: æ¨ç†éªŒè¯
- [ ] Text Dropout åœ¨ eval æ¨¡å¼ä¸‹ç¦ç”¨
- [ ] ç¼ºå¤±æŒ‡ç¤ºç¬¦æ­£ç¡®æŒ‡å¯¼èåˆæƒé‡
- [ ] æ¨ç†é€Ÿåº¦åœ¨é¢„æœŸèŒƒå›´å†…

---

## ğŸš€ å‘½ä»¤è¡Œä½¿ç”¨ç¤ºä¾‹

### å¯ç”¨æè¿°æ–‡æœ¬çš„è®­ç»ƒå‘½ä»¤
```bash
# Gating èåˆ (æ¨èé¦–é€‰)
deepspeed --include localhost:0,1,2,3 fine_tuning.py \
    --batch-size 16 --epochs 20 --dataset CSL_Daily \
    --use_descriptions \
    --text_fusion_type gating \
    --text_dropout_p 0.1 \
    --rgb_support \
    --finetune out/stage2/best.pth

# Cross-Attention èåˆ (é«˜æ€§èƒ½ï¼ŒPhase 2)
deepspeed --include localhost:0,1,2,3 fine_tuning.py \
    --batch-size 8 --epochs 20 --dataset CSL_Daily \
    --use_descriptions \
    --text_fusion_type cross_attn \
    --text_dropout_p 0.15 \
    --rgb_support \
    --finetune out/stage2/best.pth
```

### ç¦ç”¨æè¿°æ–‡æœ¬çš„è®­ç»ƒå‘½ä»¤ (å‘åå…¼å®¹)
```bash
deepspeed --include localhost:0,1,2,3 fine_tuning.py \
    --batch-size 16 --epochs 20 --dataset CSL_Daily \
    --rgb_support \
    --finetune out/stage2/best.pth
# è¾“å‡ºåº”ä¸åŸæœ‰æ¨¡å‹ç›¸åŒ
```

---

## ğŸ“š æ–‡ä»¶å…³è”å…³ç³»å›¾

```
temporal_alignment.py
    â”œâ”€ DescriptionLoader (è¯»å–description/)
    â””â”€ TemporalAligner (æ™ºèƒ½æ’å€¼)
         â†“ (è¢«è°ƒç”¨)
datasets.py
    â”œâ”€ S2T_Dataset.__init__ (åˆå§‹åŒ–åŠ è½½å™¨)
    â”œâ”€ S2T_Dataset.__getitem__ (åŠ è½½å’Œå¯¹é½æè¿°)
    â””â”€ collate_fn (æ‰“åŒ…æ‰¹é‡æ•°æ®)
         â†“ (å‘ä¸Šæ¸¸æä¾›å¤šæ¨¡æ€æ•°æ®)
text_fusion_modules.py
    â”œâ”€ TextEncoder (mT5ç¼–ç )
    â”œâ”€ GatingFusion (èåˆ)
    â””â”€ LearnableMaskEmbedding (æ©ç )
         â†“ (è¢«æ¨¡å‹è°ƒç”¨)
models.py
    â””â”€ Uni_Sign (é›†æˆç¼–ç å™¨å’Œèåˆ)
         â†“ (è¢«è®­ç»ƒå¾ªç¯è°ƒç”¨)
fine_tuning.py
    â””â”€ train_epoch / eval_step (è®­ç»ƒå’ŒéªŒè¯)
         â†“ (å‚æ•°ç”±ä»¥ä¸‹æä¾›)
utils.py & config.py (å‘½ä»¤è¡Œå’Œé…ç½®)
```

---

## ğŸ“Œ é‡è¦æé†’

1. **å‘åå…¼å®¹**: `--use_descriptions=False` æ—¶ï¼Œæ•´ä¸ªæè¿°å¤„ç†è·¯å¾„åº”è¢«è·³è¿‡ï¼Œæ¨¡å‹è¡¨ç°åº”ä¸åŸæœ‰ç›¸åŒ
2. **æ˜¾å­˜ç®¡ç†**: mT5 å’Œ Gating å¯èƒ½å¢åŠ æ˜¾å­˜å ç”¨ï¼Œå»ºè®®ä»å°batch_sizeå¼€å§‹
3. **Text Dropout**: ä»…åœ¨è®­ç»ƒæ—¶åº”ç”¨ï¼Œæ¨ç†æ—¶éœ€ç¦ç”¨
4. **ç¼ºå¤±å¤„ç†**: ä¿è¯æ‰€æœ‰ä»£ç è·¯å¾„éƒ½èƒ½å¤„ç† `descriptions=None` çš„æƒ…å†µ
5. **æµ‹è¯•ä¼˜å…ˆ**: æ¯æ­¥ä¿®æ”¹åéƒ½åº”è¿›è¡Œå•ç‹¬çš„å•å…ƒæµ‹è¯•

---

## ğŸ”— å…³è”æ–‡æ¡£å‚è€ƒ

- Stage 3 æ€»ä½“: `mds/README_OPTIMIZATION.md`
- è®¾è®¡ç»†èŠ‚: `mds/MULTIMODAL_FUSION_DESIGN.md`
- ä¼ªä»£ç å‚è€ƒ: `mds/PSEUDOCODE_REFERENCE.md`
- æ‰§è¡ŒæŒ‡å—: `mds/EXECUTION_SUMMARY.md`

**ä¸‹ä¸€æ­¥**: ç­‰å¾…ä¿®æ”¹æŒ‡ä»¤çš„ç»†åŒ–æˆ–é€ä¸ªä»£ç å®ç°çš„å…·ä½“è¦æ±‚ã€‚
