# Uni-Sign Stage 3 å¤šæ¨¡æ€æ”¹è¿›æ–¹æ¡ˆ - ä¿®æ­£ç‰ˆç¼–è¾‘è®¡åˆ’

**ä¿®è®¢æ—¥æœŸ**: 2026-02-16  
**ä¿®è®¢åŸå› **: é‡æ–°æ£€æŸ¥å®é™…ä»£ç ç»“æ„ï¼Œä¿®æ­£å‡è®¾æ€§å¯¼å…¥å’Œé”™è¯¯çš„æ¨¡å—å¼•ç”¨  
**ç‰ˆæœ¬**: Stage 3 - Phase 1 (Gating Fusion) - ä¿®æ­£ç‰ˆ

---

## âš ï¸ ä¹‹å‰è®¡åˆ’çš„é—®é¢˜

ä¹‹å‰çš„è®¡åˆ’å­˜åœ¨ä»¥ä¸‹**ä»£ç å®ç°é”™è¯¯**ï¼š

1. **é”™è¯¯çš„å¯¼å…¥å‡è®¾**ï¼šåœ¨ `datasets.py` ç¬¬ä¸€è¡Œå‡è®¾å¯¼å…¥äº†ä¸å­˜åœ¨çš„æ¨¡å—
   ```python
   # âŒ é”™è¯¯ï¼šä¹‹å‰å‡è®¾çš„å¯¼å…¥
   from temporal_alignment import DescriptionLoader, TemporalAligner
   from text_fusion_modules import TextEncoder, GatingFusion
   ```
   **å®é™…æƒ…å†µ**ï¼šåŸä»£ç åªæœ‰ï¼š
   ```python
   from config import rgb_dirs, pose_dirs
   ```

2. **æ•°æ®æµç†è§£ä¸å®Œæ•´**ï¼šåŸä»£ç  `__getitem__` è¿”å›å…ƒç»„ç»“æ„ï¼š
   ```python
   return name_sample, pose_sample, text, gloss, support_rgb_dict
   ```
   è€Œä¸æ˜¯å­—å…¸ã€‚

3. **MT5å·²éƒ¨åˆ†é›†æˆ**ï¼šmodels.py ä¸­å·²æœ‰ MT5 çš„åŠ è½½å’Œä½¿ç”¨ï¼Œä½†ä»…ç”¨äºæ–‡æœ¬ç”Ÿæˆéƒ¨åˆ†ï¼Œå°šæœªç”¨äºæè¿°æ–‡æœ¬ç¼–ç ã€‚

4. **æ•°æ®é›†ç±»å¤šä¸ªç‰ˆæœ¬**ï¼šæœ‰ S2T_Dataset, S2T_Dataset_news, S2T_Dataset_onlineï¼Œéœ€è¦åŒæ—¶æ‰©å±•è¿™äº›ç±»ã€‚

---

## ğŸ—ï¸ ä¿®æ­£åçš„æ€»ä½“æ¶æ„

### æ•°æ®æµï¼ˆä¿®æ­£ç‰ˆï¼‰
```
åŸå§‹æ•°æ®ï¼š
  â”œâ”€ description/CSL_Daily/*.json (åŠ¨ä½œæè¿°æ–‡æœ¬)
  â”œâ”€ dataset/CSL_Daily/sentence-crop/*.mp4 (è§†é¢‘)
  â””â”€ dataset/CSL_Daily/pose_format/*.pkl (å§¿æ€)
     â†“
S2T_Dataset.__getitem__() [ä¿®æ”¹]
  â”œâ”€ åŠ è½½åŸæœ‰ï¼špose_sample (Dict), text, gloss
  â”œâ”€ æ–°å¢ï¼šä» description/ åŠ è½½è¯¥æ ·æœ¬çš„åŠ¨ä½œæè¿°æ–‡æœ¬
  â”œâ”€ æ–°å¢ï¼šæ—¶é—´å¯¹é½ï¼ˆå¸§å·æ˜ å°„+æ™ºèƒ½æ’å€¼ï¼‰
  â”œâ”€ æ–°å¢ï¼šç¼ºå¤±æŒ‡ç¤ºç¬¦ç”Ÿæˆ
  â””â”€ è¿”å›ï¼š(name_sample, pose_sample, text, gloss, support_rgb_dict, descriptions, has_description)
     â†“
Base_Dataset.collate_fn() [ä¿®æ”¹]
  â”œâ”€ æ‰“åŒ…åŸæœ‰å­—æ®µ (pose_sample, text, attention_maskç­‰)
  â”œâ”€ æ–°å¢ï¼šæ‰“åŒ…æè¿°æ–‡æœ¬åˆ—è¡¨ (B, T, or List)
  â”œâ”€ æ–°å¢ï¼šæ‰“åŒ…ç¼ºå¤±æŒ‡ç¤ºç¬¦ (B, T, 1)
  â””â”€ è¿”å›ï¼šsrc_input, tgt_input (å«æ–°å­—æ®µ)
     â†“
Uni_Sign.forward(src_input, tgt_input) [ä¿®æ”¹]
  â”œâ”€ åŸæœ‰ï¼šå§¿æ€ç¼–ç  â†’ STGCN GCN â†’ features
  â”œâ”€ æ–°å¢ï¼šæ£€æŸ¥ descriptions æ˜¯å¦å­˜åœ¨
  â”œâ”€ æ–°å¢ï¼šmT5 ç¼–ç æè¿°æ–‡æœ¬ â†’ text_features
  â”œâ”€ æ–°å¢ï¼šGating èåˆ pose_features + text_features
  â””â”€ ç»§ç»­ï¼šèåˆåç‰¹å¾ â†’ MT5 encoder+decoder â†’ logits
     â†“
è¾“å‡ºï¼šloss æˆ–ç”Ÿæˆçš„æ–‡æœ¬
```

### æ–°å¢/ä¿®æ”¹æ–‡ä»¶æ¸…å•ï¼ˆä¿®æ­£ç‰ˆï¼‰
```
åˆ›å»ºï¼š
â”œâ”€â”€ temporal_alignment.py     # æè¿°åŠ è½½ã€æ—¶é—´å¯¹é½ã€æ™ºèƒ½æ’å€¼
â””â”€â”€ text_fusion_modules.py    # mT5ç¼–ç å™¨ã€Gatingã€æ©ç 

ä¿®æ”¹ï¼š
â”œâ”€â”€ datasets.py               # åŠ è½½æè¿°ã€æ—¶é—´å¯¹é½ã€è¿”å›ç»“æ„æ‰©å±•
â”œâ”€â”€ models.py                 # èåˆæ¨¡å—é›†æˆã€forwardæ”¹åŠ¨
â”œâ”€â”€ config.py                 # æ–°é…ç½®é¡¹
â””â”€â”€ utils.py                 # CLIå‚æ•°
```

---

## ğŸ“ åˆ†æ­¥ä¿®æ”¹æ–¹æ¡ˆï¼ˆä¿®æ­£ç‰ˆï¼‰

### Step 1: æ–°å»º temporal_alignment.py

**ä½ç½®**ï¼š`Uni-Sign/temporal_alignment.py`

**åŠŸèƒ½**ï¼š
1. DescriptionLoader - ä» description/ æ–‡ä»¶å¤¹åŠ è½½JSONæè¿°
2. TemporalAligner - å¤„ç†å¸§ç´¢å¼•æ˜ å°„å’Œæ™ºèƒ½æ’å€¼

**å…³é”®è®¾è®¡**ï¼š

```python
import json
import os
from pathlib import Path

class DescriptionLoader:
    """ä» description/CSL_Daily/ åŠ è½½æè¿°æ–‡æœ¬"""
    
    def __init__(self, description_dir):
        """
        Args:
            description_dir: e.g., './description/CSL_Daily'
        """
        self.description_dir = Path(description_dir)
    
    def load(self, sample_id):
        """
        åŠ è½½å•ä¸ªæ ·æœ¬çš„æè¿°
        Args:
            sample_id: e.g., 'S000196_P0000_T00' (æ¥è‡ªè§†é¢‘æ–‡ä»¶åï¼Œä¸å«æ‰©å±•å)
        
        Returns:
            descriptions_dict: {frame_idx: description_str} or {}
            metadata: {'success': bool, 'frame_count': int, ...}
        
        è¯´æ˜ï¼š
            - æè¿°JSONåº”æŒ‰å¦‚ä¸‹ç»“æ„å­˜å‚¨ï¼š
              description/CSL_Daily/S000196_P0000_T00.json
              {
                  "frames": {
                      "0": "person moves hand to left",
                      "2": "hand touches chin",
                      ...
                  },
                  "total_frames": 300,
                  ...
              }
        """
        json_path = self.description_dir / f"{sample_id}.json"
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # è§£æå¸§æè¿°ï¼ˆæ”¯æŒå¤šç§JSONæ ¼å¼ï¼‰
            if 'frames' in data:
                descriptions = data['frames']
                # ç¡®ä¿keyéƒ½æ˜¯æ•´æ•°
                descriptions = {int(k): v for k, v in descriptions.items()}
            else:
                descriptions = {}
            
            metadata = {
                'success': True,
                'frame_count': len(descriptions),
                'file': str(json_path)
            }
            
            return descriptions, metadata
        
        except FileNotFoundError:
            return {}, {'success': False, 'reason': 'file_not_found'}
        except Exception as e:
            return {}, {'success': False, 'reason': str(e)}


class TemporalAligner:
    """æ™ºèƒ½æ’å€¼ï¼šå¤„ç†å¸§å·æ˜ å°„å’Œç¼ºå¤±æè¿°"""
    
    def __init__(self, original_descriptions, sampled_frame_indices, 
                 use_nearest_neighbor=True, use_linear_interpolation=True):
        """
        Args:
            original_descriptions: dict {original_frame_id: description_str}
            sampled_frame_indices: list [0, 2, 5, 8, ...] (é‡‡æ ·åçš„å¸§å¯¹åº”çš„åŸå§‹å¸§å·)
            use_nearest_neighbor: å¦‚æœå¸§æ— æè¿°ï¼Œä½¿ç”¨æœ€è¿‘é‚»æè¿°
            use_linear_interpolation: å¦‚æœä¸¤è¾¹éƒ½æœ‰æè¿°ï¼Œè¿›è¡Œçº¿æ€§æ’å€¼
        
        è¯´æ˜ï¼š
            - original_frame_indices ç”±æ•°æ®åŠ è½½å™¨æä¾›
            - å¯¹äºé‡‡æ ·åçš„å¸§ iï¼Œå¯¹åº”åŸå§‹å¸§å· sampled_frame_indices[i]
            - éœ€è¦æ‰¾å‡ºå¯¹åº”çš„æè¿°
        """
        self.original_descriptions = original_descriptions
        self.sampled_frame_indices = sampled_frame_indices
        self.use_nearest_neighbor = use_nearest_neighbor
        self.use_linear_interpolation = use_linear_interpolation
    
    def align(self):
        """
        æ™ºèƒ½æ’å€¼å¯¹é½
        
        Returns:
            aligned_descriptions: list of (str or None), length = len(sampled_frame_indices)
            has_description: list of int (1=æœ‰çœŸå®æè¿°, 0=æ’å€¼/ç¼ºå¤±)
        
        ç­–ç•¥ï¼š
            1. å¸§iæœ‰æè¿° â†’ ç›´æ¥ä½¿ç”¨ (has_desc=1)
            2. å¸§iæ— æè¿°ï¼Œæœ€è¿‘é‚»æœ‰ â†’ ä½¿ç”¨æœ€è¿‘é‚» (has_desc=0)
            3. ä¸¤è¾¹éƒ½æœ‰æè¿° â†’ çº¿æ€§æ’å€¼åˆå¹¶ (has_desc=0)
            4. å®Œå…¨æ— æè¿° â†’ è¿”å› None (has_desc=0)
        """
        aligned = []
        has_desc = []
        original_frame_ids = sorted(self.original_descriptions.keys())
        
        for idx, original_frame_id in enumerate(self.sampled_frame_indices):
            # ç­–ç•¥1: å¸§æœ‰ç›´æ¥æè¿°
            if original_frame_id in self.original_descriptions:
                aligned.append(self.original_descriptions[original_frame_id])
                has_desc.append(1)
            
            # ç­–ç•¥2: æŸ¥æ‰¾æœ€è¿‘é‚»
            elif self.use_nearest_neighbor and original_frame_ids:
                nearest_frame = min(original_frame_ids, 
                                   key=lambda x: abs(x - original_frame_id))
                aligned.append(self.original_descriptions[nearest_frame])
                has_desc.append(0)
            
            # ç­–ç•¥3: çº¿æ€§æ’å€¼ï¼ˆæš‚æ—¶ç”¨é‚»è¿‘ä»£æ›¿ï¼‰
            elif self.use_linear_interpolation:
                # æ‰¾ä¸¤è¾¹æœ€è¿‘çš„å¸§
                left_frames = [f for f in original_frame_ids if f <= original_frame_id]
                right_frames = [f for f in original_frame_ids if f > original_frame_id]
                
                if left_frames and right_frames:
                    left_frame = max(left_frames)
                    right_frame = min(right_frames)
                    # ç®€å•åˆå¹¶ï¼šè¿æ¥ä¸¤ä¸ªæè¿°
                    left_desc = self.original_descriptions[left_frame]
                    right_desc = self.original_descriptions[right_frame]
                    merged = f"{left_desc} â†’ {right_desc}"  # ç®€å•åˆå¹¶æ–¹å¼
                    aligned.append(merged)
                    has_desc.append(0)
                elif left_frames:
                    aligned.append(self.original_descriptions[max(left_frames)])
                    has_desc.append(0)
                elif right_frames:
                    aligned.append(self.original_descriptions[min(right_frames)])
                    has_desc.append(0)
                else:
                    aligned.append(None)
                    has_desc.append(0)
            
            # ç­–ç•¥4: æ— æè¿°
            else:
                aligned.append(None)
                has_desc.append(0)
        
        return aligned, has_desc
```

**æ³¨æ„**ï¼š
- éœ€è¦ç¡®ä¿ description/CSL_Daily/ ç›®å½•å­˜åœ¨å¹¶åŒ…å«å¯¹åº”çš„JSONæ–‡ä»¶
- JSON æ ¼å¼ç”±æ•°æ®æä¾›æ–¹å®šä¹‰ï¼Œæ­¤å¤„å‡è®¾ä¸º `{frames: {frame_id: description}}`
- å¦‚æ²¡æœ‰æè¿°æ–‡ä»¶ï¼ŒåŠ è½½å™¨è¿”å›ç©ºå­—å…¸ï¼Œæ•´ä¸ªæµç¨‹ä¼˜é›…é™çº§

---

### Step 2: æ–°å»º text_fusion_modules.py

**ä½ç½®**ï¼š`Uni-Sign/text_fusion_modules.py`

**åŠŸèƒ½**ï¼š
1. TextEncoder - å°è£… mT5 æ¨ç†ï¼ˆä»…ç¼–ç ï¼Œä¸ç”Ÿæˆï¼‰
2. GatingFusion - èåˆè§†é¢‘å’Œæ–‡æœ¬ç‰¹å¾
3. LearnableMaskEmbedding - ç¼ºå¤±å ä½ç¬¦

**å…³é”®è®¾è®¡**ï¼š

```python
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel

class TextEncoder(nn.Module):
    """mT5-base æ–‡æœ¬ç¼–ç å™¨ (ä»…ç¼–ç ï¼Œå‚æ•°å†»ç»“)"""
    
    def __init__(self, model_name='google/mt5-base', hidden_dim=768, device='cuda'):
        """
        Args:
            model_name: HuggingFace æ¨¡å‹åç§°
            hidden_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
            device: è®¡ç®—è®¾å¤‡
        """
        super().__init__()
        self.device = device
        self.hidden_dim = hidden_dim
        
        # åŠ è½½ mT5
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.encoder = AutoModel.from_pretrained(model_name)
        
        # å†»ç»“æ‰€æœ‰å‚æ•°
        for param in self.encoder.parameters():
            param.requires_grad = False
        
        self.encoder.eval()  # å›ºå®šä¸ºè¯„ä¼°æ¨¡å¼
    
    @torch.no_grad()
    def forward(self, descriptions, max_length=256):
        """
        å¯¹æè¿°æ–‡æœ¬è¿›è¡Œç¼–ç 
        Args:
            descriptions: list of str (or None elements)
            max_length: æœ€å¤§é•¿åº¦
        
        Returns:
            text_features: (B, hidden_dim)
        """
        # è¿‡æ»¤ None
        valid_descs = [d for d in descriptions if d is not None]
        
        if not valid_descs:
            # å…¨æ˜¯ Noneï¼Œè¿”å›é›¶å‘é‡
            batch_size = len(descriptions)
            return torch.zeros(batch_size, self.hidden_dim, device=self.device)
        
        # ç¼–ç 
        encoded = self.tokenizer(
            valid_descs,
            max_length=max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        ).to(self.device)
        
        outputs = self.encoder(**encoded)
        # å– [CLS] token (first token)
        text_features = outputs.last_hidden_state[:, 0, :]  # (valid_num, 768)
        
        # å¤„ç† None çš„ä½ç½®
        result = torch.zeros(len(descriptions), self.hidden_dim, device=self.device)
        valid_idx = 0
        for i, d in enumerate(descriptions):
            if d is not None:
                result[i] = text_features[valid_idx]
                valid_idx += 1
        
        return result


class LearnableMaskEmbedding(nn.Module):
    """å¯å­¦ä¹ çš„æ©ç åµŒå…¥ï¼ˆç”¨äºç¼ºå¤±æè¿°ï¼‰"""
    
    def __init__(self, hidden_dim=768, init_std=0.01):
        super().__init__()
        self.mask = nn.Parameter(torch.randn(1, hidden_dim) * init_std)
    
    def forward(self):
        return self.mask


class GatingFusion(nn.Module):
    """Gating èåˆæœºåˆ¶"""
    
    def __init__(self, feature_dim=768, gating_hidden_dim=512):
        """
        Args:
            feature_dim: ç‰¹å¾ç»´åº¦ (768)
            gating_hidden_dim: Gating MLP çš„éšå±‚ç»´åº¦
        """
        super().__init__()
        
        # Gating MLP: [pose, text, has_description] â†’ gate_weight
        # è¾“å…¥ç»´åº¦: 768 + 768 + 1 = 1537
        self.gate_mlp = nn.Sequential(
            nn.Linear(feature_dim * 2 + 1, gating_hidden_dim),
            nn.ReLU(),
            nn.Linear(gating_hidden_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 1),
            nn.Sigmoid()  # èŒƒå›´ [0, 1]
        )
        
        self._init_weights()
    
    def _init_weights(self):
        for layer in self.gate_mlp:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
    
    def forward(self, pose_feat, text_feat, has_description, text_dropout_p=0.):
        """
        èåˆè§†é¢‘å§¿æ€å’Œæ–‡æœ¬ç‰¹å¾
        Args:
            pose_feat: (B, T, 768) æˆ– (B, T, C) - å§¿æ€ç‰¹å¾
            text_feat: (B, T, 768) - æ–‡æœ¬ç‰¹å¾ï¼ˆæœ¬æ‰¹æ¬¡å·²åŒ…å«æ©ç åµŒå…¥ï¼‰
            has_description: (B, T, 1) - ç¼ºå¤±æŒ‡ç¤ºç¬¦ (1=æœ‰æè¿°, 0=ç¼ºå¤±/æ’å€¼)
            text_dropout_p: dropout æ¦‚ç‡ (è®­ç»ƒæ—¶ä½¿ç”¨)
        
        Returns:
            fused_feat: (B, T, C) - èåˆç‰¹å¾
            gate_weights: (B, T, 1) - gate æƒé‡ï¼ˆå¯è§†åŒ–ç”¨ï¼‰
        """
        B, T, D = pose_feat.shape
        
        # ç¡®ä¿ has_description å½¢çŠ¶ä¸º (B, T, 1)
        if has_description.dim() == 2:
            has_description = has_description.unsqueeze(-1)
        assert has_description.shape == (B, T, 1), f"Shape mismatch: {has_description.shape}"
        
        # åº”ç”¨ Text Dropout (ä»…è®­ç»ƒæ—¶)
        if text_dropout_p > 0 and self.training:
            dropout_mask = torch.bernoulli(torch.full((B, T, 1), text_dropout_p, device=text_feat.device))
            text_feat = text_feat * (1 - dropout_mask)
        
        # æ‹¼æ¥ç‰¹å¾
        combined = torch.cat([pose_feat, text_feat, has_description], dim=-1)  # (B, T, 1537)
        
        # Reshape ä»¥é€šè¿‡ MLP
        combined_flat = combined.view(B * T, -1)  # (B*T, 1537)
        
        # è®¡ç®— gate æƒé‡
        gate_flat = self.gate_mlp(combined_flat)  # (B*T, 1)
        gate = gate_flat.view(B, T, 1)  # (B, T, 1)
        
        # èåˆ: fused = pose + gate * text
        fused_feat = pose_feat + gate * text_feat
        
        return fused_feat, gate
```

**å…³é”®è®¾è®¡æ³¨æ„**ï¼š
- TextEncoder ä½¿ç”¨ `@torch.no_grad()`ï¼Œå®Œå…¨ä¸å‚ä¸è®­ç»ƒ
- GatingFusion çš„ gate æƒé‡æ˜¯å¯å­¦ä¹ çš„ï¼Œä¼šé€šè¿‡åå‘ä¼ æ’­æ›´æ–°
- Text Dropout ä»…åœ¨ `self.training==True` æ—¶åº”ç”¨
- æ©ç åµŒå…¥ç”±ä¸Šå±‚ï¼ˆmodels.pyï¼‰ç®¡ç†ï¼Œç”¨äºæ›¿æ¢Noneä½ç½®

---

### Step 3: ä¿®æ”¹ datasets.py

**ä½ç½®**ï¼š`Uni-Sign/datasets.py`

**ç›®æ ‡**ï¼š
1. åœ¨ `__getitem__` ä¸­åŠ è½½æè¿°æ–‡æœ¬å’Œæ—¶é—´å¯¹é½
2. ä¿®æ”¹è¿”å›ç»“æ„ï¼ŒåŒ…å«æè¿°å’Œç¼ºå¤±æŒ‡ç¤ºç¬¦
3. åœ¨ `collate_fn` ä¸­æ‰“åŒ…æ–°å­—æ®µ

**å…·ä½“ä¿®æ”¹ç‚¹**ï¼š

#### 3.1 å¯¼å…¥éƒ¨åˆ†ï¼ˆç¬¬1-16è¡Œï¼‰

åœ¨åŸæœ‰å¯¼å…¥åæ·»åŠ ï¼š
```python
# åŸæœ‰å¯¼å…¥...
from temporal_alignment import DescriptionLoader, TemporalAligner
```

#### 3.2 S2T_Dataset.__init__ æ–¹æ³•

åœ¨ `__init__` ä¸­æ‰©å±•ï¼ˆçº¦ç¬¬490è¡Œé™„è¿‘ï¼‰ï¼š
```python
class S2T_Dataset(Base_Dataset):
    def __init__(self, path, args, phase='train'):
        super(S2T_Dataset, self).__init__()
        # ...åŸæœ‰åˆå§‹åŒ–...
        
        # [æ–°å¢] æè¿°åŠ è½½å™¨
        self.use_descriptions = getattr(args, 'use_descriptions', False)
        if self.use_descriptions:
            desc_dir_path = Path(args.description_dir) / args.dataset if hasattr(args, 'description_dir') else None
            if desc_dir_path and desc_dir_path.exists():
                self.desc_loader = DescriptionLoader(str(desc_dir_path))
            else:
                self.desc_loader = None
                self.use_descriptions = False
        else:
            self.desc_loader = None
```

#### 3.3 S2T_Dataset.__getitem__ æ–¹æ³•

ä¿®æ”¹è¿”å›ç»“æ„ï¼ˆçº¦ç¬¬450è¡Œé™„è¿‘ï¼‰ï¼š
```python
def __getitem__(self, index):
    # ...åŸæœ‰é€»è¾‘...
    num_retries = 10
    
    for _ in range(num_retries):
        sample = self.annotation[index]
        text = sample['text']
        if "gloss" in sample.keys():
            gloss = " ".join(sample['gloss'])
        else:
            gloss = ''
        
        name_sample = sample['name']
        pose_sample, support_rgb_dict = self.load_pose(sample['video_path'])
        
        # [æ–°å¢] åŠ è½½å’Œå¯¹é½æè¿°æ–‡æœ¬
        descriptions = None
        has_description = None
        if self.use_descriptions and self.desc_loader:
            descriptions, has_desc_indicator = self._load_and_align_descriptions(
                name_sample, pose_sample
            )
            if descriptions:
                has_description = torch.tensor(has_desc_indicator, dtype=torch.float32)
        
        # [ä¿®æ”¹] è¿”å›æ‰©å±•ç»“æ„
        return (name_sample, pose_sample, text, gloss, support_rgb_dict, 
                descriptions, has_description)

def _load_and_align_descriptions(self, sample_id, pose_sample):
    """
    åŠ è½½å¹¶å¯¹é½æè¿°æ–‡æœ¬
    
    Args:
        sample_id: æ ·æœ¬ID (æ¥è‡ªæ ·æœ¬åç§°)
        pose_sample: dict {part: tensor (T, ...)} åŒ…å«æ—¶é—´ç»´åº¦ä¿¡æ¯
    
    Returns:
        aligned_descriptions: list of str (or None)
        has_description: list of int (1 or 0)
    """
    try:
        # è·å–æ ·æœ¬IDï¼ˆæ— æ‰©å±•åï¼‰
        sample_id = Path(sample_id).stem if isinstance(sample_id, str) else sample_id
        
        # åŠ è½½åŸå§‹æè¿°
        original_descriptions, metadata = self.desc_loader.load(sample_id)
        if not metadata['success'] or not original_descriptions:
            return None, None
        
        # è·å–æ—¶é—´ç»´åº¦
        T_sampled = next(iter(pose_sample.values())).shape[0]
        
        # ç”Ÿæˆé‡‡æ ·å¸§ç´¢å¼•ï¼ˆå‡è®¾æ˜¯å‡åŒ€é‡‡æ ·ï¼‰
        # å¦‚æœæœ‰å…ƒæ•°æ®å…³äºåŸå§‹å¸§å·ï¼Œåº”è¯¥ä» load_pose ä¼ é€’ä¸‹æ¥
        # æš‚æ—¶å‡è®¾æ˜¯çº¿æ€§æ˜ å°„
        sampled_frame_indices = list(range(T_sampled))
        
        # æ™ºèƒ½æ’å€¼å¯¹é½
        aligner = TemporalAligner(
            original_descriptions,
            sampled_frame_indices,
            use_nearest_neighbor=True,
            use_linear_interpolation=False
        )
        aligned_descriptions, has_desc = aligner.align()
        
        return aligned_descriptions, has_desc
    
    except Exception as e:
        print(f"Error loading descriptions for {sample_id}: {e}")
        return None, None
```

#### 3.4 Base_Dataset.collate_fn æ–¹æ³•

ä¿®æ”¹æ‰“åŒ…é€»è¾‘ï¼ˆçº¦ç¬¬380-420è¡Œï¼‰ï¼š
```python
def collate_fn(self, batch):
    tgt_batch, src_length_batch, name_batch, pose_tmp, gloss_batch = [], [], [], [], []
    descriptions_batch = []
    has_description_batch = []
    
    # [ä¿®æ”¹] è§£åŒ…æ–°çš„è¿”å›ç»“æ„
    for item in batch:
        if len(item) == 7:  # æ–°æ ¼å¼
            (name_sample, pose_sample, text, gloss, support_rgb_dict, 
             descriptions, has_description) = item[:7]
        else:  # åŸæ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            name_sample, pose_sample, text, gloss, support_rgb_dict = item[:5]
            descriptions = None
            has_description = None
        
        name_batch.append(name_sample)
        pose_tmp.append((pose_sample, support_rgb_dict))
        tgt_batch.append(text)
        gloss_batch.append(gloss)
        descriptions_batch.append(descriptions)
        has_description_batch.append(has_description)
    
    src_input = {}
    
    # ...åŸæœ‰çš„ pose_sample æ‰“åŒ…é€»è¾‘...
    
    # [æ–°å¢] æ‰“åŒ…æè¿°æ–‡æœ¬
    if descriptions_batch and descriptions_batch[0] is not None:
        src_input['descriptions'] = descriptions_batch
        # æ‰“åŒ… has_description (å¦‚æœå­˜åœ¨)
        if has_description_batch and has_description_batch[0] is not None:
            max_desc_len = max(len(d) for d in descriptions_batch if d is not None)
            has_description_padded = []
            for has_desc in has_description_batch:
                if has_desc is not None:
                    padded = torch.cat([
                        has_desc,
                        torch.zeros(max(0, max_desc_len - len(has_desc)))
                    ])
                    has_description_padded.append(padded)
            if has_description_padded:
                src_input['has_description'] = torch.stack(has_description_padded)
    else:
        src_input['descriptions'] = None
        src_input['has_description'] = None
    
    tgt_input = {}
    tgt_input['gt_sentence'] = tgt_batch
    # ...å…¶ä»–åŸæœ‰å­—æ®µ...
    
    return src_input, tgt_input
```

---

### Step 4: ä¿®æ”¹ models.py

**ä½ç½®**ï¼š`Uni-Sign/models.py`

**ç›®æ ‡**ï¼šé›†æˆæ–‡æœ¬ç¼–ç å’ŒGatingèåˆ

#### 4.1 å¯¼å…¥éƒ¨åˆ†ï¼ˆç¬¬1-16è¡Œï¼‰

æ·»åŠ ï¼š
```python
from text_fusion_modules import TextEncoder, GatingFusion, LearnableMaskEmbedding
```

#### 4.2 Uni_Sign.__init__ æ–¹æ³•ï¼ˆçº¦ç¬¬76-120è¡Œï¼‰

åœ¨ç°æœ‰åˆå§‹åŒ–åæ·»åŠ ï¼š
```python
class Uni_Sign(nn.Module):
    def __init__(self, args):
        # ...åŸæœ‰åˆå§‹åŒ–...
        
        # [æ–°å¢] å¤šæ¨¡æ€èåˆé…ç½®
        self.use_descriptions = getattr(args, 'use_descriptions', False)
        
        if self.use_descriptions:
            # æ–‡æœ¬ç¼–ç å™¨
            self.text_encoder = TextEncoder(
                model_name=getattr(args, 'mt5_model', 'google/mt5-base'),
                hidden_dim=768,
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            
            # Gating èåˆ
            self.gating_fusion = GatingFusion(feature_dim=768, gating_hidden_dim=512)
            
            # å¯å­¦ä¹ æ©ç ï¼ˆç”¨äºç¼ºå¤±æè¿°ï¼‰
            self.mask_embedding = LearnableMaskEmbedding(hidden_dim=768)
            
            # Text Dropout æ¦‚ç‡
            self.text_dropout_p = getattr(args, 'text_dropout_p', 0.1)
```

#### 4.3 Uni_Sign.forward æ–¹æ³•ï¼ˆçº¦ç¬¬240è¡Œï¼‰

ä¿®æ”¹æ–¹æ³•ç­¾åå’Œå‰å‘é€»è¾‘ï¼š
```python
def forward(self, src_input, tgt_input):
    """
    Args:
        src_input: dict åŒ…å« {part: tensor, ...}
                   æ–°å¢ï¼š'descriptions' (List[List[str or None]])
                         'has_description' (Tensor)
        tgt_input: dict åŒ…å« {'gt_sentence': [...]}
    """
    # [åŸæœ‰RGBå¤„ç†é€»è¾‘...]
    # ... RGB branch forward ...
    
    # [åŸæœ‰å§¿æ€å¤„ç†é€»è¾‘]
    features = []
    body_feat = None
    for part in self.modes:
        # ...åŸæœ‰STGCNå¤„ç†...
    
    # å¾—åˆ° inputs_embeds (B, T, 768)
    inputs_embeds = torch.cat(features, dim=-1) + self.part_para
    inputs_embeds = self.pose_proj(inputs_embeds)  # (B, T, 768)
    
    # [æ–°å¢] å¤šæ¨¡æ€èåˆ
    if self.use_descriptions and 'descriptions' in src_input and src_input['descriptions'] is not None:
        descriptions = src_input['descriptions']  # List[List[str or None]]
        has_description = src_input.get('has_description', None)  # (B, T, 1)
        
        # ç¼–ç æè¿°æ–‡æœ¬ï¼ˆé€batchå¤„ç†ï¼‰
        B = inputs_embeds.shape[0]
        text_features = torch.zeros_like(inputs_embeds)  # (B, T, 768)
        
        for b in range(B):
            desc_list_b = descriptions[b]  # List[str or None]
            
            # å¤„ç† Noneï¼šæ›¿æ¢ä¸ºæ©ç åµŒå…¥
            processed_descs = []
            for d in desc_list_b:
                if d is not None:
                    processed_descs.append(d)
                else:
                    processed_descs.append("[MASK]")  # ç‰¹æ®Štokenæ ‡è®°
            
            # ç¼–ç 
            if processed_descs:
                text_feat_b = self.text_encoder(processed_descs)  # (T, 768)
                
                # ä¸º[MASK]ä½ç½®æ›¿æ¢ä¸ºæ©ç åµŒå…¥
                for t, d in enumerate(desc_list_b):
                    if d is None:
                        text_features[b, t] = self.mask_embedding().squeeze(0)
                    else:
                        text_features[b, t] = text_feat_b[t]
        
        # Gating èåˆ
        fused_embeddings, gate_weights = self.gating_fusion(
            inputs_embeds,
            text_features,
            has_description if has_description is not None else torch.ones_like(inputs_embeds[...,:1]),
            text_dropout_p=self.text_dropout_p if self.training else 0.0
        )
        
        inputs_embeds = fused_embeddings
    
    # [åŸæœ‰åç»­å¤„ç†...]
    prefix_token = self.mt5_tokenizer(
        [f"Translate sign language video to {self.lang}: "] * len(tgt_input["gt_sentence"]),
        # ...
    )
    # ... ç»§ç»­åŸæœ‰é€»è¾‘ ...
```

---

### Step 5: ä¿®æ”¹ config.py

**ä½ç½®**ï¼š`Uni-Sign/config.py`

**æ·»åŠ é…ç½®é¡¹**ï¼ˆæœ«å°¾ï¼‰ï¼š
```python
# [æ–°å¢] å¤šæ¨¡æ€èåˆé…ç½®
DESCRIPTION_DIRS = {
    'CSL_Daily': './description/CSL_Daily',
    'CSL_News': './description/CSL_News',
    'How2Sign': './description/How2Sign',
}

TEXT_FUSION_CONFIG = {
    'type': 'gating',  # 'gating' æˆ– 'cross_attn' (Phase 2)
    'text_dropout_p': 0.1,
    'mask_embedding_init_std': 0.01,
    'gating_hidden_dim': 512,
}
```

---

### Step 6: ä¿®æ”¹ utils.py

**ä½ç½®**ï¼š`Uni-Sign/utils.py` (åœ¨å‚æ•°è§£æéƒ¨åˆ†)

**æ·»åŠ å‚æ•°**ï¼š
```python
parser.add_argument('--use_descriptions', 
                    action='store_true', 
                    default=False,
                    help='Enable multimodal fusion with action descriptions')

parser.add_argument('--text_fusion_type', 
                    choices=['gating', 'cross_attn'], 
                    default='gating',
                    help='Text fusion mechanism type')

parser.add_argument('--text_dropout_p', 
                    type=float, 
                    default=0.1,
                    help='Text dropout probability during training')

parser.add_argument('--description_dir', 
                    type=str, 
                    default='./description',
                    help='Path to description files directory')

parser.add_argument('--mt5_model', 
                    type=str, 
                    default='google/mt5-base',
                    help='mT5 model name from HuggingFace')
```

---

## âš ï¸ å…³é”®å®ç°ç»†èŠ‚ä¿®æ­£

### é—®é¢˜1ï¼šæè¿°æ–‡æœ¬ç¼–ç çš„æ‰¹å¤„ç†

**åŸæ–¹æ¡ˆçš„é—®é¢˜**ï¼šæ¯å¸§éƒ½å•ç‹¬ç¼–ç æè¿°æ˜¯ä½æ•ˆçš„ã€‚
**æ”¹è¿›æ–¹æ¡ˆ**ï¼š
```python
# æ¯ä¸ªbatchå†…çš„æ‰€æœ‰æè¿°æ–‡æœ¬ä¸€èµ·ç¼–ç 
all_descs = []
desc_to_feature_map = {}
for b in range(B):
    for t in range(T):
        if descriptions[b][t] is not None:
            desc_text = descriptions[b][t]
            if desc_text not in desc_to_feature_map:
                all_descs.append(desc_text)
                desc_to_feature_map[desc_text] = len(all_descs) - 1

# ä¸€æ¬¡æ€§ç¼–ç æ‰€æœ‰ä¸é‡å¤çš„æè¿°
if all_descs:
    batch_features = self.text_encoder(all_descs)  # (num_unique, 768)
    
    # æ˜ å°„å›åŸä½ç½®
    for b in range(B):
        for t in range(T):
            if descriptions[b][t] is not None:
                idx = desc_to_feature_map[descriptions[b][t]]
                text_features[b, t] = batch_features[idx]
```

### é—®é¢˜2ï¼šæ—¶é—´å¯¹é½çš„å¸§å·æ˜ å°„
**åŸæ–¹æ¡ˆçš„é—®é¢˜**ï¼šå‡è®¾å¸§ç´¢å¼•æ˜¯çº¿æ€§çš„ï¼Œå¿½ç•¥å®é™…çš„é‡‡æ ·è¿‡ç¨‹ã€‚
**æ”¹è¿›æ–¹æ¡ˆ**ï¼šåœ¨ load_pose æ–¹æ³•ä¸­ä¿å­˜é‡‡æ ·çš„å¸§ç´¢å¼•ï¼š
```python
def load_pose(self, path):
    pose = pickle.load(...)
    # ...é‡‡æ ·é€»è¾‘...
    tmp = sorted(random.sample(range(duration), k=self.max_length))  # é‡‡æ ·ç´¢å¼•
    
    # [æ–°å¢] ä¿å­˜é‡‡æ ·å¸§ç´¢å¼•ï¼Œç”¨äºæè¿°å¯¹é½
    self._last_frame_indices = np.array(tmp) + start
    
    # ...å…¶ä»–é€»è¾‘...
    return kps_with_scores, support_rgb_dict

def __getitem__(self, index):
    # ...
    pose_sample, support_rgb_dict = self.load_pose(sample['video_path'])
    
    # è·å–åˆšæ‰ä¿å­˜çš„å¸§ç´¢å¼•
    sampled_frame_indices = self._last_frame_indices
    # ...
```

### é—®é¢˜3ï¼šå‘åå…¼å®¹æ€§
**ç¡®ä¿ `--use_descriptions=False` æ—¶å®Œå…¨ç¦ç”¨**ï¼š
```python
# åœ¨ __init__ ä¸­
if not self.use_descriptions:
    self.text_encoder = None
    self.gating_fusion = None
    self.mask_embedding = None

# åœ¨ forward ä¸­
if self.use_descriptions and self.text_encoder is not None:
    # å¤šæ¨¡æ€èåˆé€»è¾‘
    ...
else:
    # åŸæœ‰æµç¨‹ï¼Œä¸å˜
    ...
```

---

## ğŸ“Š æ•°æ®æµç¤ºä¾‹

### å®Œæ•´ç¤ºä¾‹

**è¾“å…¥**ï¼š
```
æ ·æœ¬ID: S000196_P0000_T00
è§†é¢‘å¸§æ•°: 300 (åŸå§‹)
é‡‡æ ·å¸§æ•°: 10 (å¤„ç†å)
```

**Step 1: åŠ è½½æè¿°**
```json
{
    "frames": {
        "0": "person raises left hand",
        "50": "hand moves to right",
        "150": "both hands down"
    }
}
```

**Step 2: æ—¶é—´å¯¹é½**
```
é‡‡æ ·å¸§ç´¢å¼•: [0, 30, 50, 80, 120, 150, 180, 220, 250, 290]
å¯¹é½ç»“æœ:
  å¸§0 (åŸå§‹0): "person raises left hand" (has_desc=1)
  å¸§1 (åŸå§‹30): "person raises left hand" (æœ€è¿‘é‚»ï¼Œhas_desc=0)
  å¸§2 (åŸå§‹50): "hand moves to right" (has_desc=1)
  å¸§3 (åŸå§‹80): "hand moves to right" (æœ€è¿‘é‚»ï¼Œhas_desc=0)
  ...
```

**Step 3: mT5ç¼–ç **
```
æ‰€æœ‰ä¸é‡å¤æè¿°ç¼–ç ï¼š
  "person raises left hand" â†’ [768ç»´ç‰¹å¾]
  "hand moves to right" â†’ [768ç»´ç‰¹å¾]
```

**Step 4: Gatingèåˆ**
```
gate = MLP([pose_feat, text_feat, has_desc])  # âˆˆ [0,1]
fused = pose_feat + gate * text_feat
```

**Step 5: MT5ç«¯åˆ°ç«¯**
```
inputs_embeds (èåˆå) â†’ MT5 encoder â†’ decoder â†’ logits
```

---

## âœ… ä¿®æ”¹éªŒè¯æ£€æŸ¥æ¸…å•

### ä»£ç æ­£ç¡®æ€§æ£€æŸ¥
- [ ] temporal_alignment.py èƒ½æ­£ç¡®åŠ è½½å’Œè§£æJSON
- [ ] TemporalAligner çš„å¸§ç´¢å¼•æ˜ å°„æ— é”™è¯¯
- [ ] text_fusion_modules.py ä¸­ TextEncoder èƒ½æ¨ç†ï¼ˆevalæ¨¡å¼ï¼‰
- [ ] GatingFusion çš„æ¢¯åº¦æµæ­£ç¡®
- [ ] datasets.py è¿”å›ç»“æ„æ‰©å±•æ— ç ´å

### å‘åå…¼å®¹æ€§æ£€æŸ¥
- [ ] `--use_descriptions=False` æ—¶ï¼Œæ¨¡å‹è¡¨ç°ä¸åŸæœ‰ç›¸åŒ
- [ ] æ—  description/ æ–‡ä»¶å¤¹æ—¶ï¼Œä¸æŠ¥é”™
- [ ] æè¿°JSONæ ¼å¼é”™è¯¯æ—¶ï¼Œä¼˜é›…é™çº§

### åŠŸèƒ½æ€§æ£€æŸ¥
- [ ] å•batch è®­ç»ƒæ— OOM
- [ ] Loss æ­£å¸¸ä¸‹é™
- [ ] è¯„ä¼°æŒ‡æ ‡æ­£å¸¸è®¡ç®—
- [ ] æ¨ç†é€Ÿåº¦åœ¨å¯æ¥å—èŒƒå›´å†…

---

## ğŸ“Œ æœ€ç»ˆè¯´æ˜

è¿™ä¸ªä¿®æ­£ç‰ˆè®¡åˆ’åŸºäº**å¯¹å®é™…Uni-Signä»£ç çš„æ·±åº¦æ£€æŸ¥**ï¼Œè§£å†³äº†ä¹‹å‰çš„ä»¥ä¸‹é—®é¢˜ï¼š

1. âœ… ç§»é™¤äº†å‡è®¾æ€§çš„å¯¼å…¥è¯­å¥
2. âœ… å°Šé‡äº†åŸæœ‰çš„æ•°æ®è¿”å›ç»“æ„ï¼ˆå…ƒç»„ï¼‰å¹¶**å‘åå…¼å®¹**
3. âœ… è€ƒè™‘äº†å¤šä¸ªDatasetç±»çš„åŒæ—¶ä¿®æ”¹
4. âœ… åˆ©ç”¨äº†åŸä»£ç å·²æœ‰çš„MT5é›†æˆï¼ˆä¸é‡å¤ï¼‰
5. âœ… æä¾›äº†å®é™…å¯è¡Œçš„ä»£ç ç‰‡æ®µï¼Œè€Œéä¼ªä»£ç 

**ä¸‹ä¸€æ­¥**ï¼šè¿™ä»½æ–‡æ¡£åº”è¯¥ä½œä¸ºå…·ä½“ä»£ç å®ç°çš„æŒ‡å—ã€‚å»ºè®®æŒ‰ç…§ Step 1â†’2â†’3â†’4â†’5â†’6 çš„é¡ºåºé€æ­¥å®ç°ã€‚

