# Stage 3 å®æ–½ - ä»£ç ä¼ªä»£ç å‚è€ƒ

**ç”¨é€”**: å‚è€ƒä¼ªä»£ç ï¼ŒæŒ‡å¯¼å®é™…ä»£ç ç¼–å†™  
**è¯´æ˜**: åŒ…å«æ‰€æœ‰ 4 ä¸ªæ–‡ä»¶çš„å…³é”®ä»£ç æ®µé€»è¾‘

---

## ğŸ“„ æ–‡ä»¶ 1: datasets.py - æ•°æ®åŠ è½½æ”¹åŠ¨

### ä»£ç æ®µ 1: æè¿°åŠ è½½å™¨

```python
# ============================================================================
# ä»£ç ä½ç½®: datasets.py ä¸­æ–°å¢ç±»
# åŠŸèƒ½: ä» JSON æ–‡ä»¶åŠ è½½å’Œç®¡ç†æ–‡æœ¬æè¿°
# ============================================================================

class DescriptionLoader:
    """åŠ è½½å’Œæ£€ç´¢æ–‡æœ¬æè¿°"""
    
    def __init__(self, description_root_dir):
        self.descriptions = {}  # {video_key: {frame_id: description_text}}
        self._load_all_descriptions(description_root_dir)
    
    def _load_all_descriptions(self, root_dir):
        """éå†æ‰€æœ‰ JSON æ–‡ä»¶å¹¶åŠ è½½"""
        for phase in ['train', 'dev', 'test']:  # ä¸‰ä¸ªé˜¶æ®µ
            phase_dir = f"{root_dir}/{phase}"
            # ä¼ªä»£ç : éå† phase_dir ä¸‹çš„æ‰€æœ‰ JSON æ–‡ä»¶
            for json_file in glob(f"{phase_dir}/*.json"):
                video_id = extract_video_id(json_file)
                video_key = f"{phase}/{video_id}"
                
                # è¯»å– JSON
                data = json.load(json_file)  # [{filename, description}, ...]
                
                # æ„å»ºå¸§-æè¿°æ˜ å°„
                self.descriptions[video_key] = {}
                for entry in data:
                    frame_id = entry['filename'].split('.')[0]  # "000000"
                    self.descriptions[video_key][frame_id] = entry['description']
    
    def get_description(self, video_key, frame_id):
        """è·å–ç‰¹å®šå¸§çš„æè¿°"""
        # ä¼ªä»£ç :
        if video_key not in self.descriptions:
            return None
        
        frame_key = format_frame_id(frame_id)  # "000000" æ ¼å¼
        return self.descriptions[video_key].get(frame_key, None)
```

### ä»£ç æ®µ 2: æ—¶é—´å¯¹é½å™¨

```python
# ============================================================================
# ä»£ç ä½ç½®: datasets.py ä¸­æ–°å¢ç±»
# åŠŸèƒ½: å¤„ç†å¸§é‡‡æ ·å¯¼è‡´çš„æ—¶é—´å¯¹é½é—®é¢˜
# ============================================================================

class TemporalAligner:
    """æ™ºèƒ½æ—¶é—´å¯¹é½"""
    
    def __init__(self, strategy='intelligent_interpolation'):
        self.strategy = strategy
    
    def align_descriptions(self, frame_indices, description_func):
        """
        å°†æè¿°å¯¹é½åˆ°é‡‡æ ·åçš„å¸§ä½ç½®
        
        è¾“å…¥:
          frame_indices: [10, 25, 50, ...] é‡‡æ ·å¸§çš„åŸå§‹ç´¢å¼•
          description_func: callable(frame_id) -> str or None
        
        è¾“å‡º:
          aligned_descs: [desc_at_10, desc_at_25, ...]
        """
        # ä¼ªä»£ç :
        aligned = []
        for frame_idx in frame_indices:
            desc = description_func(frame_idx)
            
            if desc is not None:
                # æƒ…å†µ 1: ç›´æ¥æœ‰æè¿°
                aligned.append(desc)
            else:
                # æƒ…å†µ 2: æ— æè¿°ï¼Œå¯»æ‰¾æœ€è¿‘çš„
                nearest_desc = self._find_nearest_description(
                    frame_idx, 
                    frame_indices, 
                    description_func
                )
                aligned.append(nearest_desc)
        
        return aligned
    
    def _find_nearest_description(self, frame_idx, frame_indices, desc_func):
        """å¯»æ‰¾æœ€è¿‘çš„æœ‰æè¿°çš„å¸§"""
        # ä¼ªä»£ç :
        best_desc = None
        min_distance = float('inf')
        
        for search_idx in range(0, 1000):  # å‘ä¸¤è¾¹æœç´¢
            # å°è¯•å·¦è¾¹
            left_idx = frame_idx - search_idx
            if left_idx >= 0:
                desc = desc_func(left_idx)
                if desc is not None and search_idx < min_distance:
                    best_desc = desc
                    min_distance = search_idx
            
            # å°è¯•å³è¾¹
            right_idx = frame_idx + search_idx
            desc = desc_func(right_idx)
            if desc is not None and search_idx < min_distance:
                best_desc = desc
                min_distance = search_idx
            
            if best_desc is not None:
                break
        
        return best_desc
```

### ä»£ç æ®µ 3: S2T_Dataset ä¿®æ”¹

```python
# ============================================================================
# ä»£ç ä½ç½®: ä¿®æ”¹ S2T_Dataset ç±»
# æ”¹åŠ¨: __init__() å’Œ __getitem__()
# ============================================================================

class S2T_Dataset:
    
    def __init__(self, ..., use_descriptions=True, text_dropout_rate=0.3):
        # åŸæœ‰åˆå§‹åŒ–ä»£ç  ...
        
        # [æ–°å¢]
        self.use_descriptions = use_descriptions
        self.text_dropout_rate = text_dropout_rate
        
        # [æ–°å¢] åˆå§‹åŒ–æè¿°åŠ è½½å™¨
        if use_descriptions:
            self.desc_loader = DescriptionLoader(
                'description/CSL-Daily/split_data'
            )
            self.temporal_aligner = TemporalAligner(
                strategy='intelligent_interpolation'
            )
    
    def __getitem__(self, idx):
        name = self.data[idx]
        
        # ä¼ªä»£ç : åŠ è½½è§†é¢‘ç‰¹å¾
        pose_dict = self.load_pose(name)
        pose_sample = pose_dict['pose']
        frame_indices = pose_dict['__frame_indices__']
        
        # ä¼ªä»£ç : åŠ è½½æ–‡æœ¬ç›®æ ‡
        text = self.load_text(name)
        gloss = self.load_gloss(name) if self.load_gloss else None
        rgb_dict = self.load_rgb(name)
        
        # [æ–°å¢] åŠ è½½æè¿°
        description = None
        has_description = False
        
        if self.use_descriptions:
            video_key = f"{self.phase}/{name}"
            
            # å®šä¹‰è·å–æè¿°çš„å‡½æ•°
            def get_desc(frame_idx):
                return self.desc_loader.get_description(video_key, frame_idx)
            
            # æ—¶é—´å¯¹é½
            aligned_descs = self.temporal_aligner.align_descriptions(
                frame_indices=frame_indices,
                description_func=get_desc
            )
            
            # åˆå¹¶æè¿°
            valid_descs = [d for d in aligned_descs if d is not None]
            if valid_descs:
                description = " ".join(valid_descs)
                has_description = True
        
        # è¿”å›å¢å¼ºçš„æ‰¹æ•°æ®
        return {
            'name': name,
            'pose_sample': pose_sample,
            'text': text,
            'gloss': gloss,
            'rgb_dict': rgb_dict,
            'description': description,  # [æ–°å¢]
            'has_description': has_description,  # [æ–°å¢]
            'frame_indices': frame_indices  # [æ–°å¢]
        }
    
    def load_pose(self, name):
        """ä¼ªä»£ç : åŠ è½½å¹¶é‡‡æ ·å§¿æ€ï¼Œè®°å½•å¸§ç´¢å¼•"""
        # ä» pickle åŠ è½½
        pose_feat = load_pickle(f"pose_path/{name}.pkl")
        duration = len(pose_feat)
        
        if self.max_length >= duration:
            pose_sample = pose_feat
            frame_indices = list(range(duration))
        else:
            # éšæœºé‡‡æ ·
            indices = np.random.choice(duration, self.max_length, replace=False)
            indices = np.sort(indices)  # æ’åºä¿è¯é¡ºåº
            pose_sample = pose_feat[indices]
            frame_indices = indices.tolist()  # [æ–°å¢]
        
        return {
            'pose': pose_sample,
            '__frame_indices__': frame_indices  # [æ–°å¢]
        }
```

---

## ğŸ“„ æ–‡ä»¶ 2: models.py - æ¨¡å‹æ”¹åŠ¨

### ä»£ç æ®µ 1: æ–‡æœ¬ç¼–ç å™¨

```python
# ============================================================================
# ä»£ç ä½ç½®: models.py ä¸­æ–°å¢ç±»
# åŠŸèƒ½: ä½¿ç”¨ mT5-base ç¼–ç æ–‡æœ¬æè¿°
# ============================================================================

class TextEncoder(nn.Module):
    """mT5-base æ–‡æœ¬ç¼–ç å™¨"""
    
    def __init__(self, model_name='mt5-base', hidden_dim=768):
        super().__init__()
        
        # ä¼ªä»£ç : åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.encoder = AutoModel.from_pretrained(model_name)
        
        # å†»ç»“å‚æ•° (ä¸è®­ç»ƒ mT5)
        for param in self.encoder.parameters():
            param.requires_grad = False
    
    def forward(self, descriptions, max_length=256):
        """
        è¾“å…¥: descriptions (list of str)
        è¾“å‡º: text_features (B, 768)
        """
        # ä¼ªä»£ç :
        if descriptions is None or all(d is None for d in descriptions):
            # å…¨æ˜¯ Noneï¼Œè¿”å›é›¶å‘é‡æˆ–ä½¿ç”¨ mask
            return None
        
        # Tokenize and encode
        encoded = self.tokenizer(
            descriptions,
            max_length=max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        with torch.no_grad():
            outputs = self.encoder(**encoded)
            # å– [CLS] token
            text_features = outputs.last_hidden_state[:, 0, :]  # (B, 768)
        
        return text_features
```

### ä»£ç æ®µ 2: Gating èåˆæ¨¡å—

```python
# ============================================================================
# ä»£ç ä½ç½®: models.py ä¸­æ–°å¢ç±»
# åŠŸèƒ½: èåˆè§†é¢‘å’Œæ–‡æœ¬ç‰¹å¾
# ============================================================================

class GatingFusion(nn.Module):
    """Gating èåˆæœºåˆ¶"""
    
    def __init__(self, feature_dim=768):
        super().__init__()
        
        # Gating MLP: [pose(768), text(768), indicator(1)] â†’ gate(1)
        self.gate_mlp = nn.Sequential(
            nn.Linear(feature_dim * 2 + 1, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 1),
            nn.Sigmoid()  # èŒƒå›´ [0, 1]
        )
    
    def forward(self, pose_feat, text_feat, has_text_indicator):
        """
        è¾“å…¥:
          pose_feat: (B, T, 768)
          text_feat: (B, T, 768)
          has_text_indicator: (B, 1) æˆ– (B, T, 1)
        
        è¾“å‡º:
          fused_feat: (B, T, 768)
        """
        # ä¼ªä»£ç :
        B, T, D = pose_feat.shape
        
        # ç¡®ä¿ indicator å½¢çŠ¶ä¸º (B, T, 1)
        if has_text_indicator.dim() == 2:
            has_text_indicator = has_text_indicator.unsqueeze(1).expand(B, T, 1)
        
        # æ‹¼æ¥ç‰¹å¾ (æ‰¹å¤„ç†)
        combined = torch.cat([pose_feat, text_feat, has_text_indicator], dim=2)
        # shape: (B, T, 1537)
        
        # Reshape ä¸º 2D ä»¥é€šè¿‡ MLP
        combined_flat = combined.view(B*T, -1)  # (B*T, 1537)
        
        # è®¡ç®— gate
        gate_flat = self.gate_mlp(combined_flat)  # (B*T, 1)
        gate = gate_flat.view(B, T, 1)  # (B, T, 1)
        
        # èåˆ: fused = pose + gate * text
        fused_feat = pose_feat + gate * text_feat
        
        return fused_feat
```

### ä»£ç æ®µ 3: Uni_Sign æ”¹åŠ¨

```python
# ============================================================================
# ä»£ç ä½ç½®: ä¿®æ”¹ Uni_Sign ç±»çš„ __init__() å’Œ forward()
# ============================================================================

class Uni_Sign(nn.Module):
    
    def __init__(self, ..., use_description=True):
        super().__init__()
        
        # åŸæœ‰åˆå§‹åŒ– ...
        
        # [æ–°å¢] æ–‡æœ¬å¤„ç†æ¨¡å—
        self.use_description = use_description
        
        if use_description:
            self.text_encoder = TextEncoder(
                model_name='mt5-base',
                hidden_dim=768
            )
            
            # å¯å­¦ä¹ æ©ç åµŒå…¥
            self.mask_embedding = nn.Parameter(
                torch.randn(1, 768) * 0.01
            )
            
            self.gating_fusion = GatingFusion(feature_dim=768)
    
    def forward(self, src_input, tgt_input, 
                description=None, has_description=None):
        """
        è¾“å…¥:
          src_input: (B, 4, T, 150) å§¿æ€ç‰¹å¾
          tgt_input: (B, text_len) ç›®æ ‡æ–‡æœ¬
          description: list of str (æˆ– list of None)
          has_description: (B,) torch.bool
        
        è¾“å‡º:
          output: æ¨¡å‹è¾“å‡º
        """
        # ä¼ªä»£ç :
        
        # ç¬¬ 1 æ­¥: æå–è§†é¢‘ç‰¹å¾
        pose_features = self.encode_pose(src_input)  # (B, T, 768)
        
        # [æ–°å¢] ç¬¬ 2 æ­¥: èåˆæ–‡æœ¬ç‰¹å¾
        if self.use_description and description is not None:
            # 2.1 ç¼–ç æ–‡æœ¬
            text_features = self.text_encoder(description)  # (B, 768) or None
            
            # 2.2 å¤„ç†ç¼ºå¤±ï¼Œç”Ÿæˆ (B, T, 768) çš„æ–‡æœ¬ç‰¹å¾
            B, T, _ = pose_features.shape
            text_features_t = torch.zeros(B, T, 768)
            
            for b in range(B):
                if text_features is not None and has_description[b]:
                    # æœ‰çœŸå®æ–‡æœ¬: å¤åˆ¶åˆ°æ‰€æœ‰æ—¶é—´æ­¥
                    text_features_t[b] = text_features[b].unsqueeze(0).expand(T, -1)
                else:
                    # æ— æ–‡æœ¬: ä½¿ç”¨æ©ç 
                    text_features_t[b] = self.mask_embedding.expand(T, -1)
            
            # 2.3 åˆ›å»ºç¼ºå¤±æŒ‡ç¤ºç¬¦
            has_text_indicator = has_description.float().unsqueeze(1)  # (B, 1)
            
            # 2.4 èåˆ
            fused_features = self.gating_fusion(
                pose_features,
                text_features_t,
                has_text_indicator
            )  # (B, T, 768)
        else:
            fused_features = pose_features
        
        # ç¬¬ 3 æ­¥: åç»­å¤„ç† (åŸæœ‰é€»è¾‘)
        output = self.decode(fused_features, tgt_input)
        
        return output
```

---

## ğŸ“„ æ–‡ä»¶ 3: fine_tuning.py - è®­ç»ƒæ”¹åŠ¨

### ä»£ç æ®µ 1: Text Dropout å®ç°

```python
# ============================================================================
# ä»£ç ä½ç½®: fine_tuning.py ä¸­çš„ train_one_epoch() å‡½æ•°
# åŠŸèƒ½: åº”ç”¨ Text Dropout æ­£åˆ™åŒ–
# ============================================================================

def train_one_epoch(model, dataloader, optimizer, criterion, device,
                    text_dropout_rate=0.3):
    """ä¼ªä»£ç : è®­ç»ƒä¸€ä¸ª epochï¼Œå« Text Dropout"""
    
    model.train()
    total_loss = 0
    
    for batch_idx, batch in enumerate(dataloader):
        # è§£åŒ…æ‰¹æ•°æ®
        pose = batch['pose_sample'].to(device)
        text = batch['text'].to(device)
        gloss = batch['gloss'].to(device)
        description = batch['description']  # list of str
        has_description = batch['has_description'].to(device)  # (B,)
        
        # [æ–°å¢] ===== Text Dropout åº”ç”¨ =====
        description_after_dropout = []
        has_description_after_dropout = []
        
        for b in range(len(description)):
            if has_description[b].item():
                # è¯¥æ ·æœ¬æœ‰æè¿°
                if torch.rand(1).item() < text_dropout_rate:
                    # ä»¥ text_dropout_rate çš„æ¦‚ç‡ä¸¢å¼ƒ
                    description_after_dropout.append(None)
                    has_description_after_dropout.append(False)
                else:
                    # ä¿ç•™æè¿°
                    description_after_dropout.append(description[b])
                    has_description_after_dropout.append(True)
            else:
                # æœ¬èº«æ— æè¿°
                description_after_dropout.append(None)
                has_description_after_dropout.append(False)
        
        has_description_after_dropout = torch.tensor(
            has_description_after_dropout,
            dtype=torch.bool
        ).to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        
        outputs = model(
            src_input=pose,
            tgt_input=text,
            description=description_after_dropout,  # [æ–°å¢]
            has_description=has_description_after_dropout  # [æ–°å¢]
        )
        
        # æŸå¤±å’Œåå‘ä¼ æ’­ (åŸæœ‰é€»è¾‘)
        loss = criterion(outputs, gloss)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(dataloader)
```

### ä»£ç æ®µ 2: DataLoader å’Œ Collate å‡½æ•°

```python
# ============================================================================
# ä»£ç ä½ç½®: fine_tuning.py ä¸­çš„æ•°æ®åŠ è½½éƒ¨åˆ†
# åŠŸèƒ½: åˆ›å»ºå¸¦è‡ªå®šä¹‰ collate çš„ DataLoader
# ============================================================================

def create_dataloaders(batch_size, num_workers=4):
    """ä¼ªä»£ç : åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = S2T_Dataset(
        phase='train',
        use_descriptions=True,  # [æ–°å¢]
        text_dropout_rate=0.3  # [æ–°å¢]
    )
    
    dev_dataset = S2T_Dataset(
        phase='dev',
        use_descriptions=True  # [æ–°å¢]
    )
    
    # [æ–°å¢] è‡ªå®šä¹‰ collate å‡½æ•°
    def custom_collate_fn(batch):
        """å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®å­—æ®µ"""
        collated = {}
        
        # Tensor å­—æ®µ
        tensor_fields = ['pose_sample', 'text', 'gloss']
        for field in tensor_fields:
            collated[field] = torch.stack([b[field] for b in batch])
        
        # åˆ—è¡¨å­—æ®µ
        collated['name'] = [b['name'] for b in batch]
        collated['description'] = [b['description'] for b in batch]
        collated['has_description'] = torch.tensor(
            [b['has_description'] for b in batch],
            dtype=torch.bool
        )
        
        # å­—å…¸å­—æ®µ (RGB)
        if 'rgb_dict' in batch[0]:
            collated['rgb_dict'] = {
                k: torch.stack([b['rgb_dict'][k] for b in batch])
                for k in batch[0]['rgb_dict'].keys()
            }
        
        return collated
    
    # åˆ›å»º DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=custom_collate_fn,  # [æ–°å¢]
        num_workers=num_workers
    )
    
    dev_loader = DataLoader(
        dev_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=custom_collate_fn,  # [æ–°å¢]
        num_workers=num_workers
    )
    
    return train_loader, dev_loader
```

---

## ğŸ“„ æ–‡ä»¶ 4: inference.py - æ¨ç†æ”¹åŠ¨

### ä»£ç æ®µ 1: æ¨ç†å‡½æ•°

```python
# ============================================================================
# ä»£ç ä½ç½®: inference.py æˆ– test.py ä¸­çš„æ¨ç†å‡½æ•°
# åŠŸèƒ½: åŒ…å«æ–‡æœ¬æè¿°çš„æ¨ç†
# ============================================================================

def inference(model, video_path, description_db=None, use_description=True, device='cuda'):
    """ä¼ªä»£ç : å•ä¸ªè§†é¢‘æ¨ç†"""
    
    model.eval()
    
    # 1. æå–è§†é¢‘ç‰¹å¾
    pose_features = extract_pose_features(video_path)  # (1, T, 150)
    pose_features = pose_features.to(device)
    
    # [æ–°å¢] 2. åŠ è½½æˆ–ç”Ÿæˆæ–‡æœ¬æè¿°
    description = None
    has_description = False
    
    if use_description:
        video_id = extract_video_id(video_path)
        
        # ä»æ•°æ®åº“æˆ–æ–‡ä»¶åŠ è½½æè¿°
        if description_db is not None:
            description = description_db.get(video_id, None)
        else:
            description_path = f"description/{video_id}.json"
            if os.path.exists(description_path):
                data = json.load(description_path)
                descriptions = [d['description'] for d in data]
                description = " ".join(descriptions)
        
        if description is not None:
            has_description = True
    
    # 3. å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(
            src_input=pose_features,
            tgt_input=None,
            description=[description] if description else [None],  # (1,)
            has_description=torch.tensor([has_description], dtype=torch.bool)
        )
    
    # 4. è§£ç è¾“å‡º
    translation_result = decode_output(outputs)
    
    return translation_result


def batch_inference(model, video_list, description_db=None, 
                   use_description=True, device='cuda'):
    """ä¼ªä»£ç : æ‰¹é‡æ¨ç†"""
    
    results = {}
    
    for video_path in video_list:
        result = inference(
            model, video_path, description_db, use_description, device
        )
        results[video_path] = result
    
    return results
```

### ä»£ç æ®µ 2: ä¸€è‡´æ€§éªŒè¯å‡½æ•°

```python
# ============================================================================
# ä»£ç ä½ç½®: inference.py ä¸­çš„éªŒè¯å‡½æ•°
# åŠŸèƒ½: éªŒè¯æ¨ç†çš„ä¸€è‡´æ€§ (æœ‰/æ— æ–‡æœ¬å·®å¼‚)
# ============================================================================

def verify_inference_consistency(model, test_video, reference_text, 
                                description_db, device='cuda'):
    """ä¼ªä»£ç : éªŒè¯ä¸€è‡´æ€§"""
    
    model.eval()
    
    # 1. æå–ç‰¹å¾å’Œæè¿°
    pose_features = extract_pose_features(test_video)
    video_id = extract_video_id(test_video)
    description = description_db.get(video_id, None)
    
    if description is None:
        print(f"Skip: no description for {video_id}")
        return None
    
    # 2. æ¨ç†ï¼šæœ‰æ–‡æœ¬ç‰ˆæœ¬
    with torch.no_grad():
        outputs_with = model(
            src_input=pose_features.to(device),
            description=[description],
            has_description=torch.tensor([True])
        )
        pred_with = decode_output(outputs_with)
        bleu_with = compute_bleu(pred_with, reference_text)
    
    # 3. æ¨ç†ï¼šæ— æ–‡æœ¬ç‰ˆæœ¬ (ä¸¢å¼ƒæè¿°)
    with torch.no_grad():
        outputs_without = model(
            src_input=pose_features.to(device),
            description=[None],
            has_description=torch.tensor([False])
        )
        pred_without = decode_output(outputs_without)
        bleu_without = compute_bleu(pred_without, reference_text)
    
    # 4. è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
    delta_bleu = abs(bleu_with - bleu_without)
    
    # KL æ•£åº¦ (è¾“å‡ºåˆ†å¸ƒ)
    probs_with = torch.softmax(outputs_with, dim=-1)
    probs_without = torch.softmax(outputs_without, dim=-1)
    kl = torch.nn.functional.kl_div(
        probs_without.log(),
        probs_with,
        reduction='mean'
    ).item()
    
    # 5. åˆ¤æ–­æ˜¯å¦é€šè¿‡
    consistency_pass = (delta_bleu < 0.02) and (kl < 0.1)
    
    return {
        'video_id': video_id,
        'bleu_with_text': bleu_with,
        'bleu_without_text': bleu_without,
        'delta_bleu': delta_bleu,
        'kl_divergence': kl,
        'consistency_pass': consistency_pass
    }
```

---

## ğŸ”‘ å…³é”®ä»£ç æ¨¡å¼æ€»ç»“

### æ¨¡å¼ 1: æè¿°çš„ä¸‰æ€å¤„ç†

```python
if has_description:
    # æƒ…å†µ A: æœ‰æ–‡æœ¬
    text_feature = text_encoder(description)  # (B, 768)
else:
    # æƒ…å†µ B: æ— æ–‡æœ¬ï¼Œä½¿ç”¨æ©ç 
    text_feature = mask_embedding.expand(B, -1)  # (B, 768)

# ä¸¤ç§æƒ…å†µç»Ÿä¸€è¿”å› (B, 768) çš„ç‰¹å¾
```

### æ¨¡å¼ 2: æ—¶é—´ç»´åº¦å¤„ç†

```python
# æ–‡æœ¬ç‰¹å¾çš„æ—¶é—´æ‰©å±•
B, T = pose_features.shape[0:2]

text_feature_t = torch.zeros(B, T, 768)
for b in range(B):
    # å¯¹æ¯ä¸ªæ ·æœ¬
    if has_description[b]:
        # æ‰©å±•æ–‡æœ¬ç‰¹å¾åˆ°æ‰€æœ‰æ—¶é—´æ­¥
        text_feature_t[b] = text_features[b].unsqueeze(0).expand(T, -1)
    else:
        text_feature_t[b] = mask_embedding.expand(T, -1)
```

### æ¨¡å¼ 3: Gating èåˆ

```python
# ä¸‰ä¸ªä¿¡å·è¾“å…¥
signal = torch.cat([pose_feat, text_feat, has_text_indicator], dim=-1)

# è®¡ç®—æƒé‡
gate = self.gate_mlp(signal)  # èŒƒå›´ [0, 1]

# åŠ æƒèåˆ
fused = pose_feat + gate * text_feat
# æˆ–: fused = (1 - gate) * pose_feat + gate * text_feat
```

### æ¨¡å¼ 4: Text Dropout

```python
# è®­ç»ƒæ—¶
if self.training:
    for b in range(batch_size):
        if has_description[b] and torch.rand(1) < dropout_rate:
            description[b] = None  # ä¸¢å¼ƒ
            has_description[b] = False

# æ¨ç†æ—¶
# ä¸åº”ç”¨ dropoutï¼Œæ­£å¸¸ä½¿ç”¨
```

---

## ğŸ’¡ å®æ–½å»ºè®®

### ä¼˜å…ˆçº§é¡ºåº

**ç¬¬ 1 ä¼˜å…ˆçº§** (å¿…é¡»å®æ–½):
1. DescriptionLoader - æ•°æ®åŠ è½½
2. TemporalAligner - æ—¶é—´å¯¹é½
3. ä¿®æ”¹ S2T_Dataset - é›†æˆæè¿°

**ç¬¬ 2 ä¼˜å…ˆçº§** (æ ¸å¿ƒåŠŸèƒ½):
4. TextEncoder - æ–‡æœ¬ç¼–ç 
5. GatingFusion - èåˆæœºåˆ¶
6. Learnable Mask - ç¼ºå¤±å¤„ç†

**ç¬¬ 3 ä¼˜å…ˆçº§** (è®­ç»ƒä¼˜åŒ–):
7. Text Dropout - æ­£åˆ™åŒ–
8. è‡ªå®šä¹‰ Collate å‡½æ•°
9. ä¿®æ”¹ train_one_epoch()

**ç¬¬ 4 ä¼˜å…ˆçº§** (æ¨ç†å’ŒéªŒè¯):
10. æ¨ç†å‡½æ•°æ”¹åŠ¨
11. ä¸€è‡´æ€§éªŒè¯å‡½æ•°

---

**æç¤º**: é€ä¸ªå®æ–½ä¸Šè¿°ä»£ç æ®µï¼Œå®Œæˆæ¯ä¸ªæ¨¡å—åè¿›è¡Œå•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ•°æ®æµç•…é€šã€‚
