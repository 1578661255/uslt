# Stage 3 å®æ–½ - å…³é”®æ£€æŸ¥è¡¨ä¸éªŒè¯æŒ‡å—

**ç›®çš„**: æä¾›å®Œæ•´çš„æ£€æŸ¥åˆ—è¡¨ï¼Œç¡®ä¿å®æ–½çš„æ­£ç¡®æ€§å’Œå®Œæ•´æ€§

---

## âœ… å‰æœŸå‡†å¤‡æ£€æŸ¥

### æ–‡æ¡£é˜…è¯»æ¸…å•
- [ ] è¯»å®Œ STAGE3_IMPLEMENTATION_PLAN.md (é¢„è®¡ 2-3 å°æ—¶)
  - [ ] ç†è§£æ€»ä½“æ¶æ„è®¾è®¡
  - [ ] ç†è§£æ•°æ®åŠ è½½æ”¹åŠ¨é€»è¾‘
  - [ ] ç†è§£æ¨¡å‹æ¶æ„æ”¹åŠ¨
  - [ ] ç†è§£è®­ç»ƒæ”¹åŠ¨
  - [ ] ç†è§£æ¨ç†æ”¹åŠ¨
  - [ ] ç†è§£å®Œæ•´æµç¨‹å›¾

- [ ] æŸ¥é˜… PSEUDOCODE_REFERENCE.md (ä¼ªä»£ç å‚è€ƒ)
  - [ ] DescriptionLoader ä¼ªä»£ç 
  - [ ] TemporalAligner ä¼ªä»£ç 
  - [ ] TextEncoder ä¼ªä»£ç 
  - [ ] GatingFusion ä¼ªä»£ç 

### ç¯å¢ƒå‡†å¤‡
- [ ] ç¡®è®¤ Python ç‰ˆæœ¬ â‰¥ 3.8
- [ ] ç¡®è®¤ PyTorch â‰¥ 1.10
- [ ] ç¡®è®¤ Transformers â‰¥ 4.20 (ç”¨äº mT5)
- [ ] ç¡®è®¤æè¿°æ•°æ®ä½ç½®: `description/CSL-Daily/split_data/{train,dev,test}/`
- [ ] ç¡®è®¤æè¿°æ•°æ®æ ¼å¼: JSON `[{filename, description}, ...]`
- [ ] ç¡®è®¤æè¿°æ•°æ®å®Œæ•´æ€§: 449 ä¸ªè§†é¢‘

```bash
# éªŒè¯å‘½ä»¤
ls -la description/CSL-Daily/split_data/train/ | head -5
ls -la description/CSL-Daily/split_data/dev/ | head -5
ls -la description/CSL-Daily/split_data/test/ | head -5
```

---

## ğŸ“ ä»£ç å®æ–½æ£€æŸ¥æ¸…å•

### Phase 1: æ•°æ®åŠ è½½æ¨¡å— (datasets.py)

#### Step 1.1: å®ç° DescriptionLoader ç±»

å®æ–½ä½ç½®: `datasets.py` æ–°å¢ç±»

æ£€æŸ¥é¡¹:
- [ ] ç±»å®šä¹‰å®Œæˆ
- [ ] `__init__()` æ–¹æ³•æ­£ç¡®
  - [ ] `self.descriptions` åˆå§‹åŒ–ä¸ºç©ºå­—å…¸
  - [ ] è°ƒç”¨ `_load_all_descriptions()`
- [ ] `_load_all_descriptions()` æ–¹æ³•æ­£ç¡®
  - [ ] éå† 'train', 'dev', 'test' ä¸‰ä¸ªé˜¶æ®µ
  - [ ] ä½¿ç”¨ glob æŸ¥æ‰¾ JSON æ–‡ä»¶
  - [ ] æ­£ç¡®è§£æ JSON æ ¼å¼
  - [ ] å¸§å·è½¬æ¢ä¸º '000000' å­—ç¬¦ä¸²æ ¼å¼
  - [ ] å­˜å‚¨ä¸º `{video_key: {frame_id: description}}`
- [ ] `get_description()` æ–¹æ³•æ­£ç¡®
  - [ ] å¤„ç† video_key ä¸å­˜åœ¨çš„æƒ…å†µ
  - [ ] å¤„ç† frame_id ä¸å­˜åœ¨çš„æƒ…å†µ
  - [ ] è¿”å› None å½“æ— æè¿°

éªŒè¯ä»£ç :
```python
# æµ‹è¯• DescriptionLoader
loader = DescriptionLoader('description/CSL-Daily/split_data')
desc = loader.get_description('train/video_001', 0)
print(f"Description: {desc}")  # åº”è¯¥è¿”å›æ–‡æœ¬æˆ– None
```

#### Step 1.2: å®ç° TemporalAligner ç±»

å®æ–½ä½ç½®: `datasets.py` æ–°å¢ç±»

æ£€æŸ¥é¡¹:
- [ ] ç±»å®šä¹‰å®Œæˆ
- [ ] `__init__()` æ–¹æ³•æ­£ç¡®
  - [ ] `self.strategy` è®¾ç½®ä¸º 'intelligent_interpolation'
- [ ] `align_descriptions()` æ–¹æ³•æ­£ç¡®
  - [ ] è¾“å…¥ frame_indices æ˜¯åˆ—è¡¨
  - [ ] è¾“å…¥ description_func æ˜¯å¯è°ƒç”¨çš„
  - [ ] è¿”å›åˆ—è¡¨ï¼Œé•¿åº¦ç­‰äº frame_indices
  - [ ] æ¯ä¸ªå…ƒç´ æ˜¯å­—ç¬¦ä¸²æˆ– None
- [ ] `_find_nearest_description()` æ–¹æ³•æ­£ç¡®
  - [ ] å‘å·¦å’Œå‘å³å„æœç´¢æœ€è¿‘æœ‰æè¿°çš„å¸§
  - [ ] è¿”å›æœ€è¿‘çš„æè¿°
  - [ ] å¤„ç†æ— æè¿°çš„æƒ…å†µ (è¿”å› None)
- [ ] å¤„ç†è¾¹ç•Œæƒ…å†µ
  - [ ] frame_id ä¸º 0
  - [ ] frame_id è¶…å‡ºèŒƒå›´
  - [ ] æ•´ä¸ªè§†é¢‘æ— æè¿°

éªŒè¯ä»£ç :
```python
# æµ‹è¯• TemporalAligner
aligner = TemporalAligner('intelligent_interpolation')
frame_indices = [10, 25, 50, 100]

def mock_desc_func(frame_id):
    return f"desc_{frame_id}" if frame_id % 2 == 0 else None

aligned = aligner.align_descriptions(frame_indices, mock_desc_func)
print(f"Aligned: {aligned}")  # åº”è¯¥å¡«å……æ‰€æœ‰ç¼ºå¤±çš„æè¿°
```

#### Step 1.3: ä¿®æ”¹ S2T_Dataset.__init__()

å®æ–½ä½ç½®: `datasets.py` ä¿®æ”¹ç°æœ‰ç±»

æ£€æŸ¥é¡¹:
- [ ] æ·»åŠ å‚æ•°: `use_descriptions=True, text_dropout_rate=0.3`
- [ ] å­˜å‚¨å‚æ•°: `self.use_descriptions`, `self.text_dropout_rate`
- [ ] æ¡ä»¶åˆå§‹åŒ–æè¿°åŠ è½½å™¨
  ```python
  if use_descriptions:
      self.desc_loader = DescriptionLoader(...)
      self.temporal_aligner = TemporalAligner(...)
  ```
- [ ] å¤„ç† use_descriptions=False çš„æƒ…å†µ (å‘åå…¼å®¹)

#### Step 1.4: ä¿®æ”¹ S2T_Dataset.__getitem__()

å®æ–½ä½ç½®: `datasets.py` ä¿®æ”¹ç°æœ‰æ–¹æ³•

æ£€æŸ¥é¡¹:
- [ ] ä¿ç•™åŸæœ‰è¿”å›çš„æ‰€æœ‰å­—æ®µ
- [ ] æ–°å¢è¿”å›å­—æ®µ:
  - [ ] `'description'`: str æˆ– None
  - [ ] `'has_description'`: bool
  - [ ] `'frame_indices'`: list of int
- [ ] è°ƒç”¨ `load_pose()` è·å– frame_indices
- [ ] è°ƒç”¨ description_loader å’Œ temporal_aligner
- [ ] åˆå¹¶å¤šä¸ªæè¿°æ—¶ä½¿ç”¨ç©ºæ ¼åˆ†éš”
- [ ] å¤„ç† use_descriptions=False çš„æƒ…å†µ

éªŒè¯ä»£ç :
```python
# æµ‹è¯• __getitem__()
dataset = S2T_Dataset(phase='train', use_descriptions=True)
sample = dataset[0]
print(f"Keys: {sample.keys()}")
# åº”è¯¥åŒ…å«: name, pose_sample, text, gloss, rgb_dict, 
#           description, has_description, frame_indices
```

#### Step 1.5: ä¿®æ”¹ S2T_Dataset.load_pose()

å®æ–½ä½ç½®: `datasets.py` ä¿®æ”¹ç°æœ‰æ–¹æ³•

æ£€æŸ¥é¡¹:
- [ ] åŸæœ‰é€»è¾‘ä¿æŒä¸å˜
- [ ] æ·»åŠ å¸§ç´¢å¼•è®°å½•
  ```python
  # ä¸é‡‡æ ·æƒ…å†µ
  frame_indices = list(range(duration))
  
  # é‡‡æ ·æƒ…å†µ
  frame_indices = tmp.tolist()
  ```
- [ ] è¿”å›å­—å…¸åŒ…å« `'__frame_indices__'`

éªŒè¯ä»£ç :
```python
# æµ‹è¯• load_pose()
pose_dict = dataset.load_pose('video_001')
assert '__frame_indices__' in pose_dict
assert len(pose_dict['pose']) == len(pose_dict['__frame_indices__'])
```

---

### Phase 2: æ¨¡å‹æ¶æ„æ”¹åŠ¨ (models.py)

#### Step 2.1: å®ç° TextEncoder ç±»

å®æ–½ä½ç½®: `models.py` æ–°å¢ç±»

æ£€æŸ¥é¡¹:
- [ ] ç»§æ‰¿ `nn.Module`
- [ ] `__init__()` æ–¹æ³•
  - [ ] åŠ è½½ AutoTokenizer
  - [ ] åŠ è½½ AutoModel (mT5-base)
  - [ ] å†»ç»“é¢„è®­ç»ƒå‚æ•°
- [ ] `forward()` æ–¹æ³•
  - [ ] æ¥æ”¶ descriptions (list of str)
  - [ ] Tokenize å’Œç¼–ç 
  - [ ] å– [CLS] token
  - [ ] è¿”å›å½¢çŠ¶ (B, 768)
  - [ ] å¤„ç† None å€¼

éªŒè¯ä»£ç :
```python
# æµ‹è¯• TextEncoder
encoder = TextEncoder('mt5-base')
descriptions = ["è¿™æ˜¯ä¸€ä¸ªæè¿°", "å¦ä¸€ä¸ªæè¿°"]
features = encoder(descriptions)
print(f"Shape: {features.shape}")  # (2, 768)
```

#### Step 2.2: å®ç° GatingFusion ç±»

å®æ–½ä½ç½®: `models.py` æ–°å¢ç±»

æ£€æŸ¥é¡¹:
- [ ] ç»§æ‰¿ `nn.Module`
- [ ] `__init__()` æ–¹æ³•
  - [ ] å®šä¹‰ gate_mlp ç½‘ç»œ
  - [ ] MLP è¾“å…¥ç»´åº¦: 768*2+1 = 1537
  - [ ] MLP è¾“å‡ºç»´åº¦: 1
  - [ ] æœ€åä¸€å±‚ä½¿ç”¨ Sigmoid
- [ ] `forward()` æ–¹æ³•
  - [ ] æ¥æ”¶ pose_feat (B, T, 768)
  - [ ] æ¥æ”¶ text_feat (B, T, 768)
  - [ ] æ¥æ”¶ has_text_indicator (B, 1 æˆ– B, T, 1)
  - [ ] å¤„ç†å½¢çŠ¶å¹¿æ’­
  - [ ] æ‹¼æ¥ç‰¹å¾
  - [ ] è®¡ç®— gate
  - [ ] èåˆ: fused = pose + gate * text
  - [ ] è¿”å›å½¢çŠ¶ (B, T, 768)

éªŒè¯ä»£ç :
```python
# æµ‹è¯• GatingFusion
fusion = GatingFusion(768)
pose = torch.randn(2, 10, 768)
text = torch.randn(2, 10, 768)
indicator = torch.tensor([[1.0], [0.0]])
fused = fusion(pose, text, indicator)
print(f"Shape: {fused.shape}")  # (2, 10, 768)
```

#### Step 2.3: ä¿®æ”¹ Uni_Sign.__init__()

å®æ–½ä½ç½®: `models.py` ä¿®æ”¹ç°æœ‰ç±»

æ£€æŸ¥é¡¹:
- [ ] æ·»åŠ å‚æ•°: `use_description=True`
- [ ] æ¡ä»¶åˆå§‹åŒ–æ–‡æœ¬æ¨¡å—
  ```python
  if use_description:
      self.text_encoder = TextEncoder(...)
      self.mask_embedding = nn.Parameter(...)
      self.gating_fusion = GatingFusion(...)
  ```
- [ ] Learnable mask åˆå§‹åŒ–
  - [ ] å½¢çŠ¶: (1, 768)
  - [ ] åˆå€¼æ¥è¿‘é›¶æˆ–éšæœº

#### Step 2.4: ä¿®æ”¹ Uni_Sign.forward()

å®æ–½ä½ç½®: `models.py` ä¿®æ”¹ç°æœ‰æ–¹æ³•

æ£€æŸ¥é¡¹:
- [ ] æ·»åŠ å‚æ•°: `description=None, has_description=None`
- [ ] ä¿ç•™åŸæœ‰å‰å‘é€»è¾‘: `pose_features = self.encode_pose(src_input)`
- [ ] æ·»åŠ æ–‡æœ¬èåˆé€»è¾‘
  ```python
  if self.use_description and description is not None:
      # ç¼–ç æ–‡æœ¬
      # å¤„ç†ç¼ºå¤±
      # èåˆ
  else:
      fused_features = pose_features
  ```
- [ ] å¤„ç† description=None çš„æƒ…å†µ
- [ ] å¤„ç† has_description çš„å½¢çŠ¶ (ç¡®ä¿ä¸º (B,) æˆ– (B, 1))

éªŒè¯ä»£ç :
```python
# æµ‹è¯• Uni_Sign.forward()
model = Uni_Sign(use_description=True)
src = torch.randn(2, 4, 256, 150)
tgt = torch.randn(2, 50)
descriptions = ["desc1", "desc2"]
has_desc = torch.tensor([True, False])
output = model(src, tgt, descriptions, has_desc)
print(f"Output shape: {output.shape}")
```

---

### Phase 3: è®­ç»ƒè„šæœ¬æ”¹åŠ¨ (fine_tuning.py)

#### Step 3.1: å®ç° Text Dropout é€»è¾‘

å®æ–½ä½ç½®: `fine_tuning.py` train_one_epoch() å‡½æ•°å†…

æ£€æŸ¥é¡¹:
- [ ] åœ¨å‰å‘ä¼ æ’­å‰åº”ç”¨ dropout
- [ ] é€æ ·æœ¬åº”ç”¨æ¦‚ç‡: `torch.rand(1).item() < text_dropout_rate`
- [ ] è®°å½• has_description çš„å˜åŒ–
- [ ] ä¸¢å¼ƒæ—¶è®¾ç½® description[b] = None ä¸” has_description[b] = False
- [ ] åªåœ¨ model.train() æ—¶åº”ç”¨ (ä¸åœ¨è¯„ä¼°æ—¶)

#### Step 3.2: å®ç° custom_collate_fn()

å®æ–½ä½ç½®: `fine_tuning.py` æ–°å¢å‡½æ•°

æ£€æŸ¥é¡¹:
- [ ] å¤„ç† Tensor å­—æ®µ (pose_sample, text, gloss)
  - [ ] ä½¿ç”¨ torch.stack()
- [ ] å¤„ç†åˆ—è¡¨å­—æ®µ (name, description)
  - [ ] ç›´æ¥è¿”å›åˆ—è¡¨
- [ ] å¤„ç†å¸ƒå°”å­—æ®µ (has_description)
  - [ ] è½¬æ¢ä¸º torch.tensor
- [ ] å¤„ç†å­—å…¸å­—æ®µ (rgb_dict)
  - [ ] é€ä¸ªé”® stack

éªŒè¯ä»£ç :
```python
# æµ‹è¯• custom_collate_fn
batch = [dataset[i] for i in range(2)]
collated = custom_collate_fn(batch)
print(f"Keys: {collated.keys()}")
# åº”è¯¥åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ
```

#### Step 3.3: ä¿®æ”¹ DataLoader åˆ›å»º

å®æ–½ä½ç½®: `fine_tuning.py` æ•°æ®åŠ è½½éƒ¨åˆ†

æ£€æŸ¥é¡¹:
- [ ] åˆ›å»º S2T_Dataset æ—¶æ·»åŠ å‚æ•°
  - [ ] `use_descriptions=True`
  - [ ] `text_dropout_rate=0.3`
- [ ] DataLoader ä½¿ç”¨ custom_collate_fn
  ```python
  DataLoader(..., collate_fn=custom_collate_fn)
  ```

#### Step 3.4: ä¿®æ”¹ train_one_epoch()

å®æ–½ä½ç½®: `fine_tuning.py` ä¿®æ”¹ç°æœ‰å‡½æ•°

æ£€æŸ¥é¡¹:
- [ ] ä» batch è§£åŒ…æ–°å­—æ®µ: description, has_description
- [ ] åº”ç”¨ Text Dropout
- [ ] è°ƒç”¨ model.forward() æ—¶ä¼ é€’æ–°å‚æ•°
  ```python
  outputs = model(..., description=..., has_description=...)
  ```

---

### Phase 4: æ¨ç†è„šæœ¬æ”¹åŠ¨ (inference.py)

#### Step 4.1: å®ç°æ¨ç†å‡½æ•°

å®æ–½ä½ç½®: `inference.py` æˆ– `test.py` æ–°å¢/ä¿®æ”¹å‡½æ•°

æ£€æŸ¥é¡¹:
- [ ] åŠ è½½æˆ–æŸ¥è¯¢æè¿°
  - [ ] ä» JSON æ–‡ä»¶è¯»å–
  - [ ] æˆ–ä»æ•°æ®åº“æŸ¥è¯¢
  - [ ] å¤„ç†ç¼ºå¤±æƒ…å†µ
- [ ] è®¾ç½® has_description æ ‡å¿—
- [ ] è°ƒç”¨ model.forward()
  ```python
  model(src_input=..., description=[...], has_description=...)
  ```
- [ ] ä½¿ç”¨ model.eval() å’Œ torch.no_grad()

#### Step 4.2: å®ç°ä¸€è‡´æ€§éªŒè¯å‡½æ•°

å®æ–½ä½ç½®: `inference.py` æ–°å¢å‡½æ•°

æ£€æŸ¥é¡¹:
- [ ] åˆ†åˆ«æ¨ç†æœ‰/æ— æ–‡æœ¬ç‰ˆæœ¬
- [ ] è®¡ç®— BLEU å·®å¼‚ (delta_bleu)
- [ ] è®¡ç®— KL æ•£åº¦
- [ ] åˆ¤æ–­æ˜¯å¦é€šè¿‡: delta_bleu < 0.02 and kl < 0.1

---

## ğŸ§ª å•å…ƒæµ‹è¯•æ£€æŸ¥æ¸…å•

### Test 1: DescriptionLoader

```python
def test_description_loader():
    loader = DescriptionLoader('description/CSL-Daily/split_data')
    
    # æµ‹è¯•æ•°æ®æ˜¯å¦åŠ è½½
    assert len(loader.descriptions) > 0, "No descriptions loaded"
    
    # æµ‹è¯•è·å–ç°å­˜æè¿°
    video_key = list(loader.descriptions.keys())[0]
    frame_key = list(loader.descriptions[video_key].keys())[0]
    desc = loader.get_description(video_key, int(frame_key))
    assert desc is not None, "Description should not be None"
    assert isinstance(desc, str), "Description should be string"
    
    # æµ‹è¯•è·å–ä¸å­˜åœ¨çš„æè¿°
    desc = loader.get_description('train/nonexistent', 0)
    assert desc is None, "Should return None for nonexistent video"
    
    print("âœ“ DescriptionLoader test passed")
```

### Test 2: TemporalAligner

```python
def test_temporal_aligner():
    aligner = TemporalAligner('intelligent_interpolation')
    
    # åˆ›å»ºæ¨¡æ‹Ÿæè¿°å‡½æ•°
    def mock_desc(frame_id):
        if frame_id % 2 == 0:
            return f"frame_{frame_id}"
        return None
    
    frame_indices = [0, 1, 2, 3, 4, 5]
    aligned = aligner.align_descriptions(frame_indices, mock_desc)
    
    assert len(aligned) == len(frame_indices), "Output length mismatch"
    assert all(d is not None for d in aligned), "Should fill all descriptions"
    
    print("âœ“ TemporalAligner test passed")
```

### Test 3: S2T_Dataset è¿”å›å€¼

```python
def test_s2t_dataset():
    dataset = S2T_Dataset(phase='train', use_descriptions=True)
    sample = dataset[0]
    
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    required_keys = ['name', 'pose_sample', 'text', 'gloss', 'rgb_dict',
                    'description', 'has_description', 'frame_indices']
    for key in required_keys:
        assert key in sample, f"Missing key: {key}"
    
    # æ£€æŸ¥å½¢çŠ¶
    assert sample['pose_sample'].shape[0] > 0
    assert isinstance(sample['description'], (str, type(None)))
    assert isinstance(sample['has_description'], bool)
    assert isinstance(sample['frame_indices'], list)
    
    print("âœ“ S2T_Dataset test passed")
```

### Test 4: TextEncoder

```python
def test_text_encoder():
    encoder = TextEncoder('mt5-base')
    
    descriptions = [
        "è¿™æ˜¯ç¬¬ä¸€ä¸ªæè¿°",
        "è¿™æ˜¯ç¬¬äºŒä¸ªæè¿°",
        None
    ]
    
    # å¤„ç† None å€¼
    valid_descs = [d if d is not None else "" for d in descriptions]
    features = encoder(valid_descs)
    
    assert features.shape == (3, 768), f"Expected (3, 768), got {features.shape}"
    
    print("âœ“ TextEncoder test passed")
```

### Test 5: GatingFusion

```python
def test_gating_fusion():
    fusion = GatingFusion(768)
    
    B, T = 4, 16  # batch_size=4, seq_len=16
    pose = torch.randn(B, T, 768)
    text = torch.randn(B, T, 768)
    indicator = torch.tensor([[1.0], [0.0], [1.0], [0.0]])
    
    fused = fusion(pose, text, indicator)
    
    assert fused.shape == (B, T, 768), f"Expected {(B, T, 768)}, got {fused.shape}"
    
    # æ£€æŸ¥ fused æ˜¯å¦åœ¨ pose å’Œ (pose + text) ä¹‹é—´
    assert torch.allclose(fused, pose, atol=1.0) or torch.allclose(fused, pose + text, atol=1.0) \
        or (fused > pose).any() and (fused < pose + text).any()
    
    print("âœ“ GatingFusion test passed")
```

### Test 6: Uni_Sign forward

```python
def test_uni_sign_forward():
    model = Uni_Sign(use_description=True)
    model.eval()
    
    B, T = 2, 256
    src = torch.randn(B, 4, T, 150)
    tgt = torch.randint(0, 1000, (B, 50))
    
    descriptions = ["æµ‹è¯•æè¿°1", None]
    has_desc = torch.tensor([True, False])
    
    with torch.no_grad():
        output = model(src, tgt, descriptions, has_desc)
    
    assert output is not None, "Output should not be None"
    print("âœ“ Uni_Sign forward test passed")
```

### Test 7: Text Dropout

```python
def test_text_dropout():
    dropout_rate = 0.3
    has_desc = torch.tensor([True, True, True, True])
    
    dropped = []
    for _ in range(100):
        new_has_desc = has_desc.clone()
        for b in range(len(has_desc)):
            if has_desc[b] and torch.rand(1).item() < dropout_rate:
                new_has_desc[b] = False
        dropped.append((~new_has_desc).sum().item())
    
    # å¹³å‡åº”è¯¥çº¦ 30% çš„æ ·æœ¬è¢«ä¸¢å¼ƒ
    avg_dropped = sum(dropped) / len(dropped)
    assert 0.2 < avg_dropped / 4 < 0.4, f"Dropout rate seems off: {avg_dropped/4}"
    
    print("âœ“ Text Dropout test passed")
```

---

## ğŸ“Š é›†æˆæµ‹è¯•æ£€æŸ¥æ¸…å•

### Integration Test 1: å®Œæ•´æ•°æ®åŠ è½½ç®¡é“

```python
def test_complete_data_pipeline():
    """æµ‹è¯•ä»æ•°æ®åŠ è½½åˆ°æ¨¡å‹è¾“å…¥çš„å®Œæ•´æµç¨‹"""
    
    # 1. åŠ è½½æ•°æ®é›†
    dataset = S2T_Dataset(phase='train', use_descriptions=True)
    
    # 2. åˆ›å»º DataLoader
    collate_fn = custom_collate_fn
    loader = DataLoader(dataset, batch_size=4, collate_fn=collate_fn)
    
    # 3. è·å–ç¬¬ä¸€ä¸ª batch
    batch = next(iter(loader))
    
    # 4. éªŒè¯ batch å½¢çŠ¶å’Œç±»å‹
    assert batch['pose_sample'].shape[0] == 4
    assert len(batch['description']) == 4
    assert len(batch['has_description']) == 4
    
    # 5. æ¨¡å‹åº”è¯¥èƒ½å¤„ç†è¿™ä¸ª batch
    model = Uni_Sign(use_description=True)
    model.eval()
    
    with torch.no_grad():
        output = model(
            src_input=batch['pose_sample'],
            tgt_input=batch['text'],
            description=batch['description'],
            has_description=batch['has_description']
        )
    
    assert output is not None
    print("âœ“ Complete data pipeline test passed")
```

### Integration Test 2: è®­ç»ƒå¾ªç¯å•æ¬¡è¿­ä»£

```python
def test_training_loop_single_iteration():
    """æµ‹è¯•è®­ç»ƒå¾ªç¯çš„å•æ¬¡è¿­ä»£"""
    
    dataset = S2T_Dataset(phase='train', use_descriptions=True)
    loader = DataLoader(dataset, batch_size=2, collate_fn=custom_collate_fn)
    
    model = Uni_Sign(use_description=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    
    batch = next(iter(loader))
    
    # Text Dropout
    for b in range(len(batch['has_description'])):
        if batch['has_description'][b] and torch.rand(1) < 0.3:
            batch['description'][b] = None
            batch['has_description'][b] = False
    
    # Forward
    output = model(
        src_input=batch['pose_sample'],
        tgt_input=batch['text'],
        description=batch['description'],
        has_description=batch['has_description']
    )
    
    # Loss
    loss = criterion(output, batch['gloss'])
    
    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    print(f"âœ“ Training loop test passed (loss: {loss.item():.4f})")
```

### Integration Test 3: ä¸€è‡´æ€§éªŒè¯

```python
def test_inference_consistency():
    """æµ‹è¯•æœ‰/æ— æ–‡æœ¬çš„æ¨ç†ä¸€è‡´æ€§"""
    
    model = Uni_Sign(use_description=True)
    model.eval()
    
    src = torch.randn(1, 4, 256, 150)
    description = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æè¿°"
    
    # æ¨ç†ï¼šæœ‰æ–‡æœ¬
    with torch.no_grad():
        output_with = model(
            src, None,
            description=[description],
            has_description=torch.tensor([True])
        )
    
    # æ¨ç†ï¼šæ— æ–‡æœ¬
    with torch.no_grad():
        output_without = model(
            src, None,
            description=[None],
            has_description=torch.tensor([False])
        )
    
    # è®¡ç®— KL æ•£åº¦
    probs_with = torch.softmax(output_with, dim=-1)
    probs_without = torch.softmax(output_without, dim=-1)
    kl = torch.nn.functional.kl_div(
        probs_without.log(), probs_with, reduction='mean'
    )
    
    print(f"âœ“ Consistency test passed (KL: {kl.item():.4f})")
    assert kl.item() < 0.5, "KL divergence too large"
```

---

## ğŸ“ˆ æ€§èƒ½åŸºå‡†æ£€æŸ¥

### Checkpoint 1: æ•°æ®åŠ è½½é€Ÿåº¦

```python
def benchmark_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½é€Ÿåº¦"""
    dataset = S2T_Dataset(phase='train', use_descriptions=True)
    
    import time
    start = time.time()
    for i in range(100):
        _ = dataset[i]
    elapsed = time.time() - start
    
    print(f"100 samples loaded in {elapsed:.2f}s ({elapsed/100*1000:.2f}ms/sample)")
    assert elapsed / 100 < 0.5, "Data loading too slow"
```

### Checkpoint 2: å‰å‘ä¼ æ’­é€Ÿåº¦

```python
def benchmark_forward():
    """æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­é€Ÿåº¦"""
    model = Uni_Sign(use_description=True)
    model.eval()
    
    src = torch.randn(4, 4, 256, 150)
    desc = ["test desc"] * 4
    has_desc = torch.ones(4)
    
    import time
    start = time.time()
    with torch.no_grad():
        for _ in range(10):
            _ = model(src, None, desc, has_desc)
    elapsed = time.time() - start
    
    print(f"10 forward passes: {elapsed:.2f}s ({elapsed/10*1000:.2f}ms/pass)")
```

---

## ğŸ¯ æœ€ç»ˆéªŒæ”¶æ ‡å‡†

å®æ–½å®Œæˆåï¼Œéœ€è¦æ»¡è¶³ä»¥ä¸‹æ ‡å‡†ï¼š

### ä»£ç è´¨é‡
- [ ] æ‰€æœ‰ä»£ç éµå¾ªé¡¹ç›®ç¼–ç è§„èŒƒ
- [ ] æ²¡æœ‰æœªä½¿ç”¨çš„å¯¼å…¥æˆ–å˜é‡
- [ ] æ‰€æœ‰å…¬å…±æ–¹æ³•éƒ½æœ‰æ–‡æ¡£å­—ç¬¦ä¸²
- [ ] é”™è¯¯å¤„ç†å®Œå–„

### åŠŸèƒ½å®Œæ•´æ€§
- [ ] æ‰€æœ‰ 4 ä¸ªå…³é”®æ¨¡å—å®ç°å®Œæˆ
- [ ] æ•°æ®åŠ è½½ç®¡é“å·¥ä½œæ­£å¸¸
- [ ] æ¨¡å‹èƒ½æ¥æ”¶å¹¶å¤„ç†æè¿°
- [ ] è®­ç»ƒå¾ªç¯åŒ…å« Text Dropout
- [ ] æ¨ç†èƒ½å¤„ç†æœ‰/æ— æ–‡æœ¬ä¸¤ç§æƒ…å†µ

### æ€§èƒ½æŒ‡æ ‡
- [ ] æ•°æ®åŠ è½½: < 500ms/sample
- [ ] å‰å‘ä¼ æ’­: < 300ms/batch (batch_size=4)
- [ ] ä¸€è‡´æ€§: KL < 0.1, BLEU diff < 0.02

### å‘åå…¼å®¹
- [ ] è®¾ç½® `use_descriptions=False` æ—¶å·¥ä½œæ­£å¸¸
- [ ] æ²¡æœ‰ç ´åç°æœ‰çš„æ¨¡å‹æ£€æŸ¥ç‚¹åŠ è½½
- [ ] ç°æœ‰çš„æ¨ç†è„šæœ¬ä»ç„¶é€‚ç”¨

---

**æœ€åæ›´æ–°**: 2026-02-14  
**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**é¡¹ç›®**: Uni-Sign Stage 3 å¤šæ¨¡æ€æ”¹è¿›
